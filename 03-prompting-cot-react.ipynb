{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7cc9910",
   "metadata": {},
   "source": [
    "# Understanding AI Prompts: Chain of Thought (CoT) vs. ReAct\n",
    "\n",
    "This guide explores two techniques for \"prompting\" Large Language Models (LLMs), which are AI systems trained on vast amounts of text data to understand and generate human-like language. We'll compare \"Chain of Thought\" (CoT) and \"ReAct\" (Reason+Act) prompting. Think of prompting as how you ask the AI to do something. The way you frame your request can significantly change the quality and accuracy of its response.\n",
    "\n",
    "For an Ops professional, understanding these techniques can be valuable when:\n",
    "* Integrating AI into monitoring or automation scripts.\n",
    "* Building tools that use AI to summarize logs or incidents.\n",
    "* Evaluating AI solutions for operational tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542bdeb7",
   "metadata": {},
   "source": [
    "## Getting Started: Setting Up Our AI Connection\n",
    "\n",
    "This first code block is about setting up the connection to an AI model. In a real-world Ops scenario, this might involve connecting to a centrally managed AI service, ensuring credentials are secure, and understanding the endpoint (the address of the AI service)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d30311d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python libraries imported and AI Client connection configured.\n"
     ]
    }
   ],
   "source": [
    "# Import necessary Python libraries\n",
    "import openai         # The official library for interacting with OpenAI models (or compatible APIs)\n",
    "import re             # Stands for 'regular expression', a tool for pattern matching in text\n",
    "import httpx          # A modern HTTP client library, used for making web requests (how we talk to the AI model)\n",
    "import os             # Provides ways to interact with the operating system (e.g., environment variables for API keys)\n",
    "import rich           # A library for creating rich text and beautiful formatting in the terminal (helps make output clearer)\n",
    "import json           # For working with JSON data, a common format for APIs\n",
    "\n",
    "# Specifically import the OpenAI client class\n",
    "from openai import OpenAI\n",
    "\n",
    "# --- Configuration for Connecting to the AI Model ---\n",
    "# Ops Perspective: These would typically be managed via configuration files,\n",
    "# environment variables, or a secrets management system in a production environment.\n",
    "\n",
    "# API Key: This is like a password that gives us access to the AI model.\n",
    "# IMPORTANT: Never hardcode real API keys in your scripts. This is a placeholder.\n",
    "api_key = \"placeholder\"  \n",
    "\n",
    "# Model Name: Specifies which AI model we want to use.\n",
    "# Different models have different capabilities (e.g., speed, accuracy, cost).\n",
    "# \"mistral-small:latest\" and \"qwen3:32b\" are examples of specific model versions.\n",
    "model = \"mistral-small:latest\"              \n",
    "\n",
    "# Base URL: This is the network address where the AI model is hosted.\n",
    "# In this case, \"http://localhost:11434/v1/\" suggests it's running locally on your machine\n",
    "# (perhaps using a tool like Ollama or a local inference server).\n",
    "# In an enterprise setup, this would be a secure, managed API endpoint.\n",
    "base_url = \"http://localhost:11434/v1/\"\n",
    "\n",
    "# --- Initialize the AI Client ---\n",
    "# This creates an 'object' (an instance of the OpenAI class) that we'll use\n",
    "# to send requests to the AI model and get responses.\n",
    "# It's configured with the URL and API key we defined above.\n",
    "client = OpenAI(\n",
    "    base_url=base_url,\n",
    "    api_key=api_key,\n",
    ")\n",
    "\n",
    "# Print a confirmation message\n",
    "print(\"Python libraries imported and AI Client connection configured.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167a4f78",
   "metadata": {},
   "source": [
    "**Ops Focus & Python/AI for Beginners:**\n",
    "\n",
    "* **`import` statements:** These lines bring in pre-written code (libraries) that provide useful functions. For example, `openai` helps talk to AI models, and `os` can help manage things like API keys (passwords for services).\n",
    "* **`api_key`:** This is like a password for the AI service. In real systems, you'd never type it directly into the code. You'd use secure methods like environment variables or a secrets vault.\n",
    "* **`model`:** AI services offer different \"models\" (like different versions of software). Some are faster, some are better at specific tasks. Choosing the right one is like picking the right tool for a job.\n",
    "* **`base_url`:** This is the web address of the AI service. `localhost` means it's running on the same machine as this script. In a company, this would be a central, managed service address.\n",
    "* **`client = OpenAI(...)`:** This line creates a \"client\" object. Think of it as setting up a dedicated communication channel to the AI service, using the address and password we specified."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff53681d",
   "metadata": {},
   "source": [
    "## Chain of Thought (CoT) Prompting\n",
    "\n",
    "### What is Chain of Thought?\n",
    "\n",
    "Imagine you're trying to solve a complex problem. You wouldn't just jump to the answer. You'd think it through, step by step. **Chain of Thought (CoT) prompting** is a way to ask an AI model to do the same thing: show its work before giving the final answer.\n",
    "\n",
    "Instead of just asking:\n",
    "`\"What's 5 + 3 * 2?\"` (which could be ambiguous)\n",
    "\n",
    "You might guide it with CoT:\n",
    "`\"First, tell me the order of operations. Then calculate 3 * 2. Finally, add 5 to that result. What's the final answer?\"`\n",
    "\n",
    "**Why is this useful, especially in Ops?**\n",
    "\n",
    "* **Better Accuracy for Complex Tasks:** If an AI needs to diagnose a multi-step failure or parse a complex log, forcing it to \"think step-by-step\" can lead to more accurate results.\n",
    "* **Transparency & Debugging:** You can see *how* the AI reached its conclusion. If it makes a mistake, you can often pinpoint where its reasoning went wrong. This is crucial for trusting AI in operational tasks.\n",
    "* **Reduces \"Hallucinations\":** Sometimes AI models confidently make up incorrect information (called \"hallucinations\"). CoT can help reduce this by making the reasoning process more explicit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c62494",
   "metadata": {},
   "source": [
    "### Few-Shot Learning (A Quick Detour)\n",
    "\n",
    "The example below uses a concept called **\"few-shot prompting.\"**\n",
    "\n",
    "* **Zero-Shot:** You ask the AI a question directly without any examples.\n",
    "    * `\"Translate 'hello' to French.\"`\n",
    "* **One-Shot:** You give the AI one example before asking your question.\n",
    "    * `\"English: goodbye -> French: au revoir. Now, English: hello -> French: ?\"`\n",
    "* **Few-Shot:** You give the AI several examples. This helps the AI better understand the pattern, format, or type of reasoning you expect. The CoT example below uses few-shot prompting by providing multiple question/thought/answer examples.\n",
    "\n",
    "From an Ops perspective, if you're trying to get an AI to classify alerts or summarize incident reports in a specific format, providing a few good examples (few-shot) can drastically improve its performance and consistency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d65d695",
   "metadata": {},
   "source": [
    "### CoT Example:\n",
    "\n",
    "This code shows how to use CoT. We give the AI a \"system prompt\" which is like setting the rules for how it should behave. Then we ask our actual question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4877fc1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: If Tom has 5 cookies and eats 2, how many does he have left?\n",
      "\n",
      "Question: If Tom has 5 cookies and eats 2. How many does he have left?\n",
      "Thought: Initially Tom had 5 cookies. After eating 2 of them, the remaining number will be obtained by subtracting 2 from 5, i.e.,    5 - 2 =3.\n",
      "Answer: 3\n"
     ]
    }
   ],
   "source": [
    "# This is the \"system prompt\" - instructions for the AI.\n",
    "# It tells the AI to always provide a \"Thought\" process before the \"Answer\".\n",
    "# It also includes several examples (few-shot learning) to show the desired format.\n",
    "cot_system_prompt = \"\"\"\n",
    "You are a thoughtful and logical assistant. For every question, you will:\n",
    "- Think step-by-step under a “Thought” section.\n",
    "- Then write the final result under “Answer”.\n",
    "- Always follow the structure shown below.\n",
    "\n",
    "Use this format:\n",
    "Question: <the question>\n",
    "Thought: <your detailed reasoning>\n",
    "Answer: <final answer>\n",
    "\n",
    "Here are some examples:\n",
    "\n",
    "Question: If a train leaves at 2 PM and takes 3 hours to reach its destination, what time does it arrive?\n",
    "Thought: The train departs at 2 PM. If it travels for 3 hours, it will arrive at 2 + 3 = 5 PM.\n",
    "Answer: 5 PM\n",
    "\n",
    "Question: What is the capital of the country whose official language is French and borders Germany?\n",
    "Thought: France is a country that borders Germany and has French as its official language. The capital of France is Paris.\n",
    "Answer: Paris\n",
    "\n",
    "Question: What is the sum of the first three even numbers?\n",
    "Thought: The first three even numbers are 2, 4, and 6. Their sum is 2 + 4 + 6 = 12.\n",
    "Answer: 12\n",
    "\n",
    "Now answer the next question using the same format:\n",
    "\"\"\"\n",
    "\n",
    "# This is the actual question we want the AI to answer.\n",
    "user_question_cot = \"If Tom has 5 cookies and eats 2, how many does he have left?\"\n",
    "\n",
    "# --- Preparing the request for the AI ---\n",
    "# AI models often work with a \"conversation history\" or a list of messages.\n",
    "# \"system\" role: Sets the overall behavior of the AI.\n",
    "# \"user\" role: Represents what the human user is asking.\n",
    "messages_for_cot = [\n",
    "    {\"role\": \"system\", \"content\": cot_system_prompt},\n",
    "    {\"role\": \"user\", \"content\": user_question_cot}\n",
    "]\n",
    "\n",
    "# --- Sending the request to the AI and getting the response ---\n",
    "# `client.chat.completions.create(...)` is the function call to the AI.\n",
    "# - `model=model`: Tells it which AI model to use (that we configured earlier).\n",
    "# - `messages=messages_for_cot`: Passes our structured request.\n",
    "completion_cot = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=messages_for_cot,\n",
    ")\n",
    "\n",
    "# --- Displaying the AI's response ---\n",
    "# The AI's answer is typically nested within the 'completion' object.\n",
    "# `completion_cot.choices[0].message.content` accesses the text of the AI's reply.\n",
    "print(f\"Question: {user_question_cot}\\n\")\n",
    "print(completion_cot.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa6c7ee",
   "metadata": {},
   "source": [
    "**Ops Focus & Python/AI for Beginners:**\n",
    "\n",
    "* **`cot_system_prompt` (String):** This is a multi-line piece of text (a \"string\" in Python) that acts as the main instruction or persona for the AI. Notice the `\"\"\"triple quotes\"\"\"` which allow text to span multiple lines.\n",
    "* **Placeholders like `<the question>`:** These aren't code; they're part of the instructions to the AI, showing it the desired output format.\n",
    "* **`messages_for_cot` (List of Dictionaries):** This is how we structure the conversation for the AI.\n",
    "    * A Python `list` is an ordered collection of items, enclosed in `[]`.\n",
    "    * A Python `dictionary` is a collection of `key: value` pairs, enclosed in `{}`. Here, keys are like `\"role\"` and `\"content\"`.\n",
    "    * `\"role\": \"system\"`: Sets the overall context or rules for the AI.\n",
    "    * `\"role\": \"user\"`: Contains the specific question from the user.\n",
    "* **`client.chat.completions.create(...)`:** This is the command to \"run\" the AI with our prompt.\n",
    "* **`completion_cot.choices[0].message.content`:** The AI's response comes back in a structured format. This specific path navigates through that structure to get the actual text of the AI's answer. It might seem complex, but it's a standard way APIs return data. You usually get a primary response (the \"choice\"), and within that, the message, and then the content of that message."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b57c8af",
   "metadata": {},
   "source": [
    "## ReAct (Reason + Act) Prompting\n",
    "\n",
    "### What is ReAct?\n",
    "\n",
    "**ReAct** takes CoT a step further. It stands for **Reason + Act**.\n",
    "\n",
    "While CoT is about the AI *thinking* step-by-step, ReAct allows the AI to:\n",
    "1.  **Reason (Thought):** Decide what it needs to know or do.\n",
    "2.  **Act (Action):** Propose an action to get that information, like using a tool (e.g., a search engine, a calculator, or even an internal company API to check a server's status).\n",
    "3.  **Observe (Observation):** Get the result from that action.\n",
    "4.  Repeat this Thought -> Action -> Observation cycle until it has enough information.\n",
    "5.  **Answer:** Provide the final answer.\n",
    "\n",
    "**Why is ReAct powerful for Ops?**\n",
    "\n",
    "* **Interacting with the Real World:** This is key! An Ops AI needs to interact with systems. ReAct provides a framework for the AI to say, \"I need to check the current CPU load on server X\" (Thought), then specify an action like `execute_command('get_cpu_load serverX')` (Action). Your system would then run this command and feed the result back to the AI (Observation).\n",
    "* **Using Tools:** You can give the AI \"tools\" – these could be scripts, API calls, or database queries. For instance:\n",
    "    * `LookupKnowledgeBase('error code 123')`\n",
    "    * `GetCurrentOnCall('web_team')`\n",
    "    * `QueryMetrics('database_latency_p99', 'last_1_hour')`\n",
    "* **Dynamic Problem Solving:** For problems where the AI needs to gather information incrementally (like troubleshooting a complex outage), ReAct is much more powerful than CoT alone.\n",
    "* **Transparency:** Like CoT, you see the AI's \"thinking\" process, including what tools it tried to use and what results it got. This is invaluable for verifying its actions and for auditing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3af70b",
   "metadata": {},
   "source": [
    "> ### ReAct vs. React (the JavaScript library)\n",
    ">\n",
    ">It's important to note: **ReAct prompting** for LLMs is different from **React**, the popular JavaScript library for building user interfaces. The names are similar, but the concepts are unrelated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1723fe42",
   "metadata": {},
   "source": [
    "### ReAct Template:\n",
    "\n",
    "The general flow of a ReAct prompt looks like this:\n",
    "\n",
    "1.  **Question:** \\[The user's initial question]\n",
    "2.  **Thought:** \\[The AI's internal reasoning about what it needs to do next]\n",
    "3.  **Action:** \\[The AI proposes an action, often involving a tool, e.g., `Search('topic')`, `Calculator(2+2)`, `APICall('get_status')`]\n",
    "4.  **Observation:** \\[The result of that action, fed back to the AI]\n",
    "    * *(...The Thought -> Action -> Observation cycle can repeat multiple times...)*\n",
    "5.  **Thought:** \\[Final reasoning before giving the answer]\n",
    "6.  **Answer:** \\[The AI's final response to the question]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18f522a",
   "metadata": {},
   "source": [
    "### ReAct Example:\n",
    "\n",
    "In this example, we simulate the \"Action\" and \"Observation\" parts. In a real system, your code would actually *execute* the action (e.g., perform a web search if the action is `Search(...)`) and then provide the real result as the observation.\n",
    "\n",
    "Here, we are still using **few-shot prompting** by giving an example of the ReAct flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6d091cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending ReAct request to the AI. This might take a few moments...\n",
      "\n",
      "I need to find out about the boundaries and topography of the eastern sector of the Colorado orogeny.\n",
      "Action: Lookup('eastern sector of the Colorado orogeny')\n",
      "Observation:\n",
      "The eastern sector of the Colorado orogeny extends into the High Plains, which is characterized by elevated plains and plateaus.\n",
      "\n",
      "Thought: I have information about topography in the area. I need to lookup elevation.\n",
      "Action: Lookup('Elevation range for High Plains')\n",
      "Observation: The elevation of the High Plains generally ranges from 1,500 to 4,900 feet\n",
      "Answer: The elevation range for the eastern sector of the Colorado orogeny is approximately from 1,500 to 4,900 feet\n"
     ]
    }
   ],
   "source": [
    "# This is a few-shot example demonstrating the ReAct format.\n",
    "# Notice the Thought -> Action -> Observation flow.\n",
    "react_few_shot_example = (\n",
    "    \"Question: What is the capital of the country that borders Germany and has Vienna as its capital?\\n\"\n",
    "    \"Thought: I need to find which country has Vienna as its capital.\\n\"\n",
    "    \"Action: Lookup('country with capital Vienna')\\n\"  # The AI suggests an action\n",
    "    \"Observation: Austria\\n\"  # This would be the *result* of running the Lookup tool\n",
    "    \"Thought: Now I need to check if Austria borders Germany.\\n\"\n",
    "    \"Action: Lookup('Does Austria border Germany?')\\n\"\n",
    "    \"Observation: Yes\\n\"\n",
    "    \"Answer: The capital of Austria, which borders Germany, is Vienna.\"\n",
    ")\n",
    "\n",
    "# A more complex question that might benefit from the ReAct approach\n",
    "# (requiring the AI to \"look up\" information in stages).\n",
    "user_question_react = \"What is the elevation range for the area that the eastern sector of the Colorado orogeny extends into?\"\n",
    "\n",
    "# --- Assembling the Prompt ---\n",
    "# In this ReAct example, the few-shot example is combined with the new question.\n",
    "# The `f\"\"` syntax is an \"f-string\" in Python. It lets you embed variables directly in a string.\n",
    "# We're telling the AI: \"Here's an example of how to think, act, and observe. Now, do the same for this new question.\"\n",
    "# The prompt ends with \"Thought:\" to encourage the AI to start its reasoning process.\n",
    "\n",
    "react_prompt_for_user = f\"\"\"{react_few_shot_example}\n",
    "\n",
    "Question: {user_question_react}\n",
    "Thought:\"\"\" # We explicitly ask the AI to start with a thought.\n",
    "\n",
    "# --- Preparing the request for the AI ---\n",
    "# System Prompt: Sets the overall ReAct behavior.\n",
    "# User Prompt: Contains the few-shot example and the new question.\n",
    "messages_for_react = [\n",
    "    {\"role\": \"system\", \"content\": \"You are an assistant that answers questions using a ReAct format: Thought -> Action -> Observation -> Answer. You can use a `Lookup('query')` action to find information.\"},\n",
    "    {\"role\": \"user\", \"content\": react_prompt_for_user}\n",
    "]\n",
    "\n",
    "# --- Sending the request to the AI ---\n",
    "# This will take a bit longer because the AI is generating a more complex, multi-step response.\n",
    "# In a Jupyter Notebook, you'd see a [*] next to the cell while it's processing.\n",
    "print(\"Sending ReAct request to the AI. This might take a few moments...\\n\")\n",
    "completion_react = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=messages_for_react,\n",
    ")\n",
    "\n",
    "# --- Displaying the AI's response ---\n",
    "print(completion_react.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54fa29f5",
   "metadata": {},
   "source": [
    "**Ops Focus & Python/AI for Beginners:**\n",
    "\n",
    "* **`react_few_shot_example` (String):** Again, a multi-line string providing an example. The key here is the `Action:` and `Observation:` parts.\n",
    "* **Simulated Actions:** In this notebook, `Lookup('...')` isn't actually running a search. The AI is *generating text* that looks like it's planning to run a search. In a real ReAct system (often called an \"AI Agent\"), you'd have Python code that:\n",
    "    1.  Parses the AI's \"Action\" (e.g., sees `Lookup('Colorado Orogeny')`).\n",
    "    2.  Actually *executes* that lookup (e.g., queries a database or a web search API).\n",
    "    3.  Takes the result and feeds it back to the AI as the \"Observation.\"\n",
    "* **`f\"\"\"...\"\"\"` (f-string):** This is a convenient way in Python to build strings that include the values of variables. ` {react_few_shot_example}` and `{user_question_react}` insert the content of those variables into the main string.\n",
    "* **`\"role\": \"system\", \"content\": \"Answer questions using a ReAct format...\"`:** This system message is crucial. It explicitly tells the AI to use the Thought-Action-Observation pattern and even suggests a tool (`Lookup`).\n",
    "* **Prompt Engineering:** The way `react_prompt_for_user` is constructed is a simple example of **prompt engineering** – carefully crafting the input to the AI to guide it towards the desired output format and reasoning process. This is a critical skill when working with LLMs. You're essentially programming the AI through the instructions you give it.\n",
    "\n",
    "**Note on the ReAct Example Style:**\n",
    "* In the CoT example, the few-shot examples were part of the *system prompt*.\n",
    "* In this ReAct example, the few-shot example is passed along with the *user's question*.\n",
    "These are just different ways to structure the information for the AI. Experimenting with how you provide examples and instructions is part of prompt engineering. For Ops, consistency in how you format these prompts can be important for predictable AI behavior.\n",
    "\n",
    "The output from the ReAct cell shows the AI trying to break down the problem:\n",
    "1.  Identify it needs info on \"Colorado Orogeny\" and its \"Eastern Extension.\"\n",
    "2.  Propose `Action: Lookup(...)` for these.\n",
    "3.  (Simulated) `Observation:` gets facts about the orogeny.\n",
    "4.  Then it reasons it needs elevation data for specific states.\n",
    "5.  Proposes another `Action: Lookup(...)` for elevations.\n",
    "6.  (Simulated) `Observation:` gets elevation ranges.\n",
    "7.  Finally, it synthesizes this into an `Answer:`.\n",
    "\n",
    "This iterative process is what makes ReAct powerful for complex queries that require external knowledge or tool use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00695f9f",
   "metadata": {},
   "source": [
    "## Chain of Thought (CoT) vs. ReAct (Reasoning + Acting): A Comparison\n",
    "\n",
    "Here’s a summary to help you decide which approach might be better for different operational tasks.\n",
    "\n",
    "### Chain of Thought (CoT)\n",
    "\n",
    "* **What it is:** Purely a reasoning process. The AI \"thinks aloud\" step-by-step before giving an answer. It doesn't interact with external tools or live data sources on its own during this process.\n",
    "* **Analogy for Ops:** Like a senior engineer verbalizing their troubleshooting steps based on their existing knowledge and documentation they've already read, without actively querying a live system during that specific explanation.\n",
    "\n",
    "| Strengths                                                                | Limitations                                                                 |\n",
    "| :----------------------------------------------------------------------- | :-------------------------------------------------------------------------- |\n",
    "| Good for tasks requiring pure logical deduction (e.g., interpreting complex configuration logic, simple math, multi-step factual recall from its training data). | Cannot interact with external tools or live systems (e.g., can't check current server status, query a live database, or get real-time alerts). |\n",
    "| Relatively easy to implement; you're just asking the model to explain its reasoning. | Limited if the answer depends on up-to-the-minute data or information not in its training. |\n",
    "| Transparent: You can see the reasoning path, which helps in understanding and debugging the AI's output. | Not suitable for tasks requiring actions in the real world or data retrieval. |\n",
    "\n",
    "### ReAct (Reasoning + Acting)\n",
    "\n",
    "* **What it is:** Combines CoT-style reasoning with the ability to take **Actions** (like calling tools, performing web searches, or executing functions) and then incorporating the **Observations** (results of those actions) into its next thought process.\n",
    "* **Analogy for Ops:** Like an automated runbook or a sophisticated troubleshooting script. The system reasons what to check next (`Thought`), executes a command or API call to check it (`Action`), gets the result (`Observation`), and then decides the next step based on that new information.\n",
    "\n",
    "| Strengths                                                                 | Limitations                                                                  |\n",
    "| :------------------------------------------------------------------------ | :--------------------------------------------------------------------------- |\n",
    "| Excellent for building \"AI Agents\" that can interact with your environment (e.g., querying monitoring systems, interacting with ticketing systems, looking up internal documentation via an API). | More complex to implement. You need to build the \"tools\" the AI can call and the framework to handle the Action/Observation loop. |\n",
    "| Enables decision-making with dynamic, real-time data.                   | Debugging can be harder if the chain of thoughts and actions becomes very long or if tools produce unexpected results. |\n",
    "| You can integrate your own custom tools, scripts, and data sources (e.g., access a CMDB, query a log aggregation platform, interact with cloud provider APIs). | Requires careful design of available tools and permissions to ensure safety and prevent unintended actions. (Ops concern: security and control are paramount). |\n",
    "| Highly effective for Retrieval Augmented Generation (RAG) systems, where the AI first \"looks up\" relevant documents before answering a question. |                                                                              |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d044aae5",
   "metadata": {},
   "source": [
    "## Summary: Which to Choose?\n",
    "\n",
    "* **Use CoT when:**\n",
    "    * The problem can be solved by reasoning alone, based on the information the AI was trained on or information provided directly in the prompt.\n",
    "    * You need to understand the AI's thought process for a relatively self-contained task.\n",
    "    * Examples: Summarizing a provided text in detail, solving a logic puzzle, explaining a concept based on general knowledge.\n",
    "    * **Ops Example:** Asking an AI to explain the potential consequences of a specific configuration change, assuming it has been given the relevant documentation or context in the prompt.\n",
    "\n",
    "* **Use ReAct when:**\n",
    "    * The AI needs to interact with external systems, tools, or dynamic data sources to answer the question or complete a task.\n",
    "    * You are building an \"AI agent\" that needs to perform actions.\n",
    "    * The problem requires information gathering from multiple steps or sources.\n",
    "    * Examples: Answering \"What's the current weather in London?\" (needs a weather API), \"Summarize the latest critical alerts from our production environment\" (needs to query a monitoring system).\n",
    "    * **Ops Examples:**\n",
    "        * Building an AI assistant that can check the health of a service by querying its metrics endpoint.\n",
    "        * Creating an AI that can look up troubleshooting steps in an internal knowledge base when given an error code.\n",
    "        * Automating the initial information gathering steps for an incident ticket by having an AI query logs, metrics, and recent deployments.\n",
    "\n",
    "For many advanced Ops use cases where AI needs to interact with your infrastructure or internal data, **ReAct (or similar agent-based architectures)** will be the more powerful and practical approach. However, understanding CoT is a good foundation, as the \"Reasoning\" part of ReAct is essentially a Chain of Thought process.\n",
    "\n",
    "This notebook has shown simple, simulated examples. Building a robust ReAct system involves:\n",
    "1.  Defining the tools the AI can use.\n",
    "2.  Writing Python (or other language) functions that implement those tools.\n",
    "3.  Creating a loop that:\n",
    "    * Sends the prompt to the AI.\n",
    "    * Parses the AI's response to see if it wants to take an \"Action.\"\n",
    "    * If yes, executes the tool and gets the \"Observation.\"\n",
    "    * Feeds the \"Observation\" back to the AI for its next \"Thought.\"\n",
    "    * Repeats until the AI provides a final \"Answer.\"\n",
    "\n",
    "This area is rapidly evolving, with frameworks like LangChain and LlamaIndex providing higher-level tools to build such agentic systems. However, understanding these fundamental prompting techniques is key to effectively using and operationalizing LLMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6fc9dc-ddf3-431f-9cda-08b6575da7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "In the next Module we will move onto tool calling - a core underlying enabler of Agentic AI"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
