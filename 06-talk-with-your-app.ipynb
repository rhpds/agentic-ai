{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8987006-614e-41b7-9c2f-aa6359e8e231",
   "metadata": {},
   "source": [
    "# Talk to your App\n",
    "\n",
    "In majority of the examples, LLM Usage is shown as a chat-bot or personal assistant. \n",
    "\n",
    "A more powerful usage of Agentic-AI are when they interact with the system in the background and enrich it or take autonomous actions. This is conceptually similar to streaming analytics - with analytics being processed by Agents powered by LLMs.\n",
    "\n",
    "You will see that for chat-bot type usage - if the AI infrastructure is down, things still work.\n",
    "However for the Agents in backend - if the AI system is down, then the system will suffer some down time.\n",
    "\n",
    "_Each module is typically dependent on the prior modules having been completed successfully_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a049faf1-8a5c-4b36-a85c-7f7627635c4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import re\n",
    "import httpx\n",
    "import os\n",
    "import rich\n",
    "import json\n",
    "from openai import OpenAI\n",
    "from agents import Agent, ModelSettings, function_tool,Runner,AsyncOpenAI,OpenAIChatCompletionsModel\n",
    "# from rich.pretty import pprint\n",
    "\n",
    "api_key = \"placeholder\" \n",
    "# model=\"mistral-small:latest\"\n",
    "\n",
    "model = \"qwen3:32b\"\n",
    "base_url = \"http://localhost:11434/v1/\"\n",
    "\n",
    "model = OpenAIChatCompletionsModel( \n",
    "    model=model,\n",
    "    openai_client=AsyncOpenAI(base_url=base_url, api_key=api_key)\n",
    ")\n",
    "\n",
    "print(\"Setup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af63f69-2d06-40f9-9fa5-cc490cf11768",
   "metadata": {},
   "source": [
    "## Automated Error Handling\n",
    "\n",
    "1.The process is started when an ansible job log completion data comes in. (as of now, we simulate this as a user input)\n",
    "1. It examimes if there is any error. If no error, it ends\n",
    "1. If there is an error:\n",
    "    - Agent analyzes and recommends\n",
    "    - Agent opens a jira ticket\n",
    "    - Agent sends a slack message\n",
    "\n",
    "![Workflow](resources/images/agent_log_workflow.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f956699-6763-4872-a4c5-8fe453c8165b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Literal\n",
    "\n",
    "from agents import Agent, ItemHelpers, Runner, TResponseInputItem, trace\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Advisor:\n",
    "    body: str\n",
    "\n",
    "@dataclass\n",
    "class Slacker:\n",
    "    body: str\n",
    "\n",
    "@dataclass\n",
    "class JIRAer:\n",
    "    body: str\n",
    "\n",
    "\n",
    "advisor_agent = Agent(\n",
    "    name=\"advisor_agent\",\n",
    "    instructions=\"You can look at the contents of an ansible log and spot the error. You will describe what the error is in a few crisp sentences so that a human can take corrective actions.\",\n",
    "    model = model,\n",
    "    output_type=Advisor,\n",
    ")\n",
    "\n",
    "slack_agent = Agent(\n",
    "    name=\"slack_agent\",\n",
    "    instructions=\"If there is an error captured in the input, you will always state - I have slacked the message. Also add the contents that in the provided input. Give a made up slack link. If there is no error, then just say - All is well, there is nothing to be done.\",\n",
    "    model = model,\n",
    "    output_type=Slacker,\n",
    ")\n",
    "\n",
    "jira_agent = Agent(\n",
    "    name=\"jira_agent\",\n",
    "    instructions=\"If there is an error captured in the input, you will always state - I have opened a JIRA ticket. Also add the contents that in the provided input. Give a made up JIRA Handle. If there is no error, then just say - All is well, there is nothing to be done.\",\n",
    "    model = model,\n",
    "    output_type=JIRAer,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f07d1120-a9f5-4727-aaff-e2865f2a85d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------Advisor Output----------\n",
      "The error message indicates that the connection to the host failed because the user's password has expired. You need to update the password for the user on the target host to resolve this issue. You can either set a new password directly on the host using the 'passwd' command or configure the user to not have their password expire using the 'chage' command. Once the password is updated, try running the Ansible playbook again.\n",
      "----------JIRA Output----------\n",
      "I have opened a JIRA ticket. Also add the contents that in the provided input. Give a made up JIRA Handle. If there is no error, then just say - All is well, there is nothing to be done.\n",
      "----------Slack Output----------\n",
      "I have slacked the message. Please refer to the link for more details: https://slack.com/messages/expired-password-error/123456789\n"
     ]
    }
   ],
   "source": [
    "#msg = \"all tasks successfully finished\"\n",
    "msg = \"could not connect to the host as the password as expired. \"\n",
    "inputs = [{\"content\": msg, \"role\": \"user\"}]\n",
    "\n",
    "#with trace(\"Router\"):\n",
    "#    story_outline_result = await Runner.run(advisor_agent,inputs)\n",
    "#    #uncomment this to see the details\n",
    "#    pprint(story_outline_result)\n",
    "#    print(\"--------------------------\")\n",
    "#    print(story_outline_result.final_output)\n",
    "\n",
    "with trace(\"Workflow\"):\n",
    "    print(\"----------Advisor Output----------\")\n",
    "    advisor_result = await Runner.run(advisor_agent,inputs)\n",
    "    print(advisor_result.final_output.body)\n",
    "    advisor_output: Advisor = advisor_result.final_output\n",
    "    \n",
    "    print(\"----------JIRA Output----------\")\n",
    "    jira_result = await Runner.run(jira_agent,advisor_output.body)\n",
    "    print(jira_result.final_output.body)\n",
    "    \n",
    "    print(\"----------Slack Output----------\")\n",
    "    slack_result = await Runner.run(slack_agent,advisor_output.body)\n",
    "    print(slack_result.final_output.body)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b9ab57-7246-4c17-a9c1-2ccc9a7cc487",
   "metadata": {},
   "source": [
    "# Human-in-the-Loop\n",
    "But LLMs hallucinate!\n",
    "\n",
    "Yes, LLMs, just like traditional AI and human beings may not always give the right answer. To handle those kind of possible mistakes, we have processes in place.\n",
    "\n",
    "In the above example, we are seamlessly blending human-in-the-loop when we open a JIRA ticket or Slack a message. Even if the contents of these are not entirely accurate, we do not lose much because a person can check and correct if needed. \n",
    "\n",
    "In distributed systems, there are lots of similar examples - which has been around us for a long time - to take care of possible errors: for example those which arise out of CAP Theorem related inconsistencies. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78716910-84ca-42b6-9f2b-faa39f4df1fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
