{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8987006-614e-41b7-9c2f-aa6359e8e231",
   "metadata": {},
   "source": [
    "# Talk to your App\n",
    "\n",
    "In majority of the examples, LLM Usage is shown as a chat-bot or personal assistant. \n",
    "\n",
    "A more powerful usage of LLM based Agents are when they interact with the system in the background and enrich it or take autonomous actions. This is conceptually similar to streaming analytics - with analytics being processed by Agents powered by LLMs.\n",
    "\n",
    "You will see that for chat-bot type usage - if the AI infrastructure is down, things still work.\n",
    "However for the Agents in backend - if the AI system is down, then the system will suffer some down time.\n",
    "\n",
    "### Assumptions\n",
    "\n",
    "* Pythons setup (I created a 3.11 venv)\n",
    "* OpenAI Key (Granite struggled but perhaps not a fair comparions ollama/grantite q4 8b v `gpt-4o`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a049faf1-8a5c-4b36-a85c-7f7627635c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import re\n",
    "import httpx\n",
    "import os\n",
    "import rich\n",
    "import json\n",
    "from openai import OpenAI\n",
    "from agents import Agent, ModelSettings, function_tool,Runner\n",
    "from rich.pretty import pprint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "201a5506-5b38-4fcb-a0c6-45e67a8ed5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boilerplate for swapping in Granite via ollama\n",
    "#model = \"granite3-dense:8b\"\n",
    "#model = \"granite3.1-dense:2b\"\n",
    "#client = OpenAI(\n",
    "#     base_url='http://localhost:11434/v1',\n",
    "#     api_key='ollama',\n",
    "# ) \n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "model = \"gpt-4o\"\n",
    "client = OpenAI() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b11b79c-4495-4f7d-b919-72bfc3fc96b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt-4o\n",
      "Certainly! Below is a simple example of a Python class called `User`. This class includes basic attributes like `username` and `email`, and a method to display user information.\n",
      "\n",
      "```python\n",
      "class User:\n",
      "    def __init__(self, username, email):\n",
      "        self.username = username\n",
      "        self.email = email\n",
      "\n",
      "    def display_user_info(self):\n",
      "        print(f\"Username: {self.username}\")\n",
      "        print(f\"Email: {self.email}\")\n",
      "\n",
      "# Example usage:\n",
      "if __name__ == \"__main__\":\n",
      "    user1 = User(\"john_doe\", \"john@example.com\")\n",
      "    user1.display_user_info()\n",
      "```\n",
      "\n",
      "### Explanation:\n",
      "- **`__init__` Method**: This is the constructor method that initializes a new instance of the `User` class with a `username` and `email`.\n",
      "- **Attributes**: `username` and `email` are instance attributes that store the user's information.\n",
      "- **`display_user_info` Method**: This method prints the user's information to the console.\n",
      "- **Example Usage**: The `if __name__ == \"__main__\":` block is used to demonstrate how to create an instance of the `User` class and call its method.\n"
     ]
    }
   ],
   "source": [
    "# Quick test code - verify LLM conenctivity etc (disable via Raw)\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Write a simple Python example class called User\"}],\n",
    "    temperature=0,\n",
    ")\n",
    "print(model)\n",
    "print(f\"{chat_completion.choices[0].message.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af63f69-2d06-40f9-9fa5-cc490cf11768",
   "metadata": {},
   "source": [
    "## Automated Error Handling\n",
    "\n",
    "1.The process is started when an ansible job log completion data comes in. (as of now, we simulate this as a user input)\n",
    "1. It examimes if there is any error. If no error, it ends\n",
    "1. If there is an error:\n",
    "    - Agent analyzes and recommends\n",
    "    - Agent opens a jira ticket\n",
    "    - Agent sends a slack message\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f956699-6763-4872-a4c5-8fe453c8165b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Literal\n",
    "\n",
    "from agents import Agent, ItemHelpers, Runner, TResponseInputItem, trace\n",
    "\n",
    "model = \"gpt-4o\"\n",
    "\n",
    "@dataclass\n",
    "class Advisor:\n",
    "    body: str\n",
    "\n",
    "@dataclass\n",
    "class Slacker:\n",
    "    body: str\n",
    "\n",
    "@dataclass\n",
    "class JIRAer:\n",
    "    body: str\n",
    "\n",
    "\n",
    "advisor_agent = Agent(\n",
    "    name=\"advisor_agent\",\n",
    "    instructions=\"You can look at the contents of an ansible log and spot the error. You will describe what the error is in a few crisp sentences so that a human can take corrective actions.\",\n",
    "    model = model,\n",
    "    output_type=Advisor,\n",
    ")\n",
    "\n",
    "slack_agent = Agent(\n",
    "    name=\"slack_agent\",\n",
    "    instructions=\"If there is an error captured in the input, you will always state - I have slacked the message. Also add the contents that in the provided input. Give a made up slack link. If there is no error, then just say - All is well, there is nothing to be done.\",\n",
    "    model = model,\n",
    "    output_type=Slacker,\n",
    ")\n",
    "\n",
    "jira_agent = Agent(\n",
    "    name=\"jira_agent\",\n",
    "    instructions=\"If there is an error captured in the input, you will always state - I have opened a JIRA ticket. Also add the contents that in the provided input. Give a made up JIRA Handle. If there is no error, then just say - All is well, there is nothing to be done.\",\n",
    "    model = model,\n",
    "    output_type=JIRAer,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f07d1120-a9f5-4727-aaff-e2865f2a85d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------Advisor Output----------\n",
      "The error indicates that Ansible is unable to connect to the host because the user's password has expired. To resolve this, update the password on the target host manually or via another management tool, ensuring compliance with security policies. Once updated, retry the connection with Ansible.\n",
      "----------JIRA Output----------\n",
      "I have opened a JIRA ticket. Details: The error indicates that Ansible is unable to connect to the host because the user's password has expired. To resolve this, update the password on the target host manually or via another management tool, ensuring compliance with security policies. Once updated, retry the connection with Ansible. \n",
      "\n",
      "JIRA Handle: ITOPS-1234\n",
      "----------Slack Output----------\n",
      "I have slacked the message: The error indicates that Ansible is unable to connect to the host because the user's password has expired. To resolve this, update the password on the target host manually or via another management tool, ensuring compliance with security policies. Once updated, retry the connection with Ansible. \n",
      "\n",
      "Link: [View Message](https://slack.com/message/expired-password-issue) \n"
     ]
    }
   ],
   "source": [
    "#msg = \"all tasks successfully finished\"\n",
    "msg = \"could not connect to the host as the password as expired. \"\n",
    "inputs = [{\"content\": msg, \"role\": \"user\"}]\n",
    "\n",
    "#with trace(\"Router\"):\n",
    "#    story_outline_result = await Runner.run(advisor_agent,inputs)\n",
    "#    #uncomment this to see the details\n",
    "#    pprint(story_outline_result)\n",
    "#    print(\"--------------------------\")\n",
    "#    print(story_outline_result.final_output)\n",
    "\n",
    "with trace(\"Workflow\"):\n",
    "    print(\"----------Advisor Output----------\")\n",
    "    advisor_result = await Runner.run(advisor_agent,inputs)\n",
    "    print(advisor_result.final_output.body)\n",
    "    advisor_output: Advisor = advisor_result.final_output\n",
    "    \n",
    "    print(\"----------JIRA Output----------\")\n",
    "    jira_result = await Runner.run(jira_agent,advisor_output.body)\n",
    "    print(jira_result.final_output.body)\n",
    "    \n",
    "    print(\"----------Slack Output----------\")\n",
    "    slack_result = await Runner.run(slack_agent,advisor_output.body)\n",
    "    print(slack_result.final_output.body)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b9ab57-7246-4c17-a9c1-2ccc9a7cc487",
   "metadata": {},
   "source": [
    "# Human-in-the-Loop\n",
    "But LLMs hallucinate!\n",
    "\n",
    "Yes, LLMs, just like traditional AI and human beings may not always give the right answer. To handle those kind of possible mistakes, we have processes in place.\n",
    "\n",
    "In the above example, we are seamlessly blending human-in-the-loop when we open a JIRA ticket or Slack a message. Even if the contents of these are not entirely accurate, we do not lose much because a person can check and correct if needed. \n",
    "\n",
    "In distributed systems, there are lots of similar examples - which has been around us for a long time - to take care of possible errors: for example those which arise out of CAP Theorem related inconsistencies. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78716910-84ca-42b6-9f2b-faa39f4df1fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".summit",
   "language": "python",
   "name": ".summit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
