{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8987006-614e-41b7-9c2f-aa6359e8e231",
   "metadata": {},
   "source": [
    "# getting started\n",
    "\n",
    "In this lab we will compare Chain of Though (Cot) and ReAct (Reason Act) Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "201a5506-5b38-4fcb-a0c6-45e67a8ed5c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports complete, Client initialized\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import re\n",
    "import httpx\n",
    "import os\n",
    "import rich\n",
    "import json\n",
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "api_key = \"placeholder\"  \n",
    "model = \"mistral-small:latest\"              # Other options here include \"qwen3:32b\"\n",
    "base_url = \"http://localhost:11434/v1/\"\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url=base_url,\n",
    "    api_key=api_key,\n",
    ")\n",
    "\n",
    "\n",
    "print(\"Imports complete, Client initialized\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9127c382-7143-41da-8723-898f392e5db1",
   "metadata": {},
   "source": [
    "# Chain of Thought"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c7476f-a86b-4f20-ae24-cf070b2493ec",
   "metadata": {},
   "source": [
    "A Chain of Thought (CoT) prompt is a prompting technique used with large language models (LLMs) where you explicitly guide the model to reason step-by-step before arriving at the final answer.\n",
    "\n",
    "Instead of asking the model to directly give you an answer, a CoT prompt encourages it to \"think aloud\"—breaking down the problem, analyzing it in stages, and only then giving a conclusion. This helps improve accuracy, especially for complex reasoning, math, logic, or multi-hop questions.\n",
    "\n",
    "Can be combined with few-shot prompting (i.e., giving multiple CoT examples before the actual question)\n",
    "\n",
    "TODO: Need to explain, as a call out, few shor learning and prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "112d49f4-f8d6-444b-8ecf-651efc85bfbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: If Tom has 5 cookies and eats 2, how many does he have left?\n",
      "\n",
      "Tom starts with 5 cookies.\n",
      "\n",
      "After eating 2 of them, we subtract the amount eated from initial\n",
      "Answer :3\n"
     ]
    }
   ],
   "source": [
    "# Static example to demonstrate format (few-shot learning)\n",
    "prompt = \"\"\"\n",
    "You are a thoughtful and logical assistant. For every question, you will:\n",
    "- Think step-by-step under a “Thought” section.\n",
    "- Then write the final result under “Answer”.\n",
    "- Always follow the structure shown below.\n",
    "\n",
    "Use this format:\n",
    "Question: <the question>\n",
    "Thought: <your detailed reasoning>\n",
    "Answer: <final answer>\n",
    "\n",
    "Here are some examples:\n",
    "\n",
    "Question: If a train leaves at 2 PM and takes 3 hours to reach its destination, what time does it arrive?\n",
    "Thought: The train departs at 2 PM. If it travels for 3 hours, it will arrive at 2 + 3 = 5 PM.\n",
    "Answer: 5 PM\n",
    "\n",
    "Question: What is the capital of the country whose official language is French and borders Germany?\n",
    "Thought: France is a country that borders Germany and has French as its official language. The capital of France is Paris.\n",
    "Answer: Paris\n",
    "\n",
    "Question: What is the sum of the first three even numbers?\n",
    "Thought: The first three even numbers are 2, 4, and 6. Their sum is 2 + 4 + 6 = 12.\n",
    "Answer: 12\n",
    "\n",
    "Now answer the next question using the same format:\n",
    "\"\"\"\n",
    "\n",
    "question = \"If Tom has 5 cookies and eats 2, how many does he have left?\"\n",
    "\n",
    "messages=[\n",
    "        {\"role\": \"system\", \"content\": prompt},\n",
    "        {\"role\": \"user\", \"content\": question}\n",
    "    ]\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=messages,\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93353aa-5f03-46ba-bbaf-20a24e963403",
   "metadata": {},
   "source": [
    "# ReAct Style\n",
    "\n",
    "TODO Callout re React v ReACT and maybe \n",
    "\n",
    "1. Encourages explicit reasoning, not just end answers\n",
    "1. Allows the model to interleave thoughts and actions\n",
    "1. Great for use with tools or plugins (e.g., search, code exec, database)\n",
    "1. Makes model behavior transparent and verifiable\n",
    "\n",
    "## Template\n",
    "\n",
    "1. Question: [user question]\n",
    "1. Thought: [model's internal reasoning]\n",
    "1. Action: [some action like Search(), Calculator(), API call]\n",
    "1. Observation: [result of the action]\n",
    "\n",
    "...repeat Thought → Action → Observation...\n",
    "\n",
    "Answer: [final response to the question]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f79fa18-b317-4614-b282-83134b135359",
   "metadata": {},
   "source": [
    "TODO: Explain/introduce \"few shot prompting\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f08e9f4-90e1-4b76-acbb-bd7d20fcc411",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Static example to demonstrate format (few-shot learning)\n",
    "\n",
    "few_shot_example = (\n",
    "    \"Question: What is the capital of the country that borders Germany and has Vienna as its capital?\\n\"\n",
    "    \"Thought: I need to find which country has Vienna as its capital.\\n\"\n",
    "    \"Action:  Lookup('country with capital Vienna')\\n\"\n",
    "    \"Observation: Austria\\n\"\n",
    "    \"Thought: Now check if Austria borders Germany.\\n\"\n",
    "    \"Action:  Lookup('Does Austria border Germany?')\\n\"\n",
    "    \"Observation: Yes\\n\"    \n",
    "    \"Answer: The capital of Austria, which borders Germany, is Vienna.\"\n",
    "    )\n",
    "\n",
    "question = \"what is elevation range for the area that the eastern sector of colorado orogeny extends into?\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840d9bad-abda-43fc-a60b-1e9102095c0a",
   "metadata": {},
   "source": [
    "Note - \n",
    "- In the last example for CoT, the system prompt got the examples.\n",
    "- In this example, we keep pass the example data with each questions.\n",
    "It is just 2 different styles to play with.\n",
    "\n",
    "\n",
    "Because the LLM is thinking ie  Thought → Action → Observation → Answer this will take a few moments \n",
    "<insert graphic of the `[*]` to the top left of the cell.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ee5e962-da45-489e-97ad-2aace3c4809c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I need to find more information about the Colorado Orogeny, its eastern boundary and the region it covers. There are two separate research areas here:\n",
      "1. The Colorado Orogeny\n",
      "2. Eastern Extension\n",
      "\n",
      "Action:\n",
      "  Lookup('Colorado Orogeny')\n",
      "  Lookup('eastern sector of colorado orogeny')\n",
      "Observation: The Colorado Orogeny is a geological event that formed the Rocky Mountains and was active around 70-45 million years ago. It affected multiple states in the western United States, The eastern extent covers parts of Wyoming, Montana North Dakota and South Dakota.\n",
      "\n",
      "Thought: Now I need to find elevation details of a region covering these four states.\n",
      "\n",
      "Action: Lookup('lowest highest point north dakota wyoming montana south dakota')\n",
      "Observation:\n",
      "Elevation range for areas in these 4 states are as follows (units are meters with feet equivalent where provided)\n",
      "- Wyoming: Highest = 13,802(fm)4206,Lowest= 920(3015f)\n",
      "-Montana: Highest = 12,799(3904f), Lowest= 1,800 (5900f))\n",
      " - South Dakota = Range 3,090.5 to 68 f) m or  16,47m-170 f)\n",
      " - North dakota: highest = 1,164(feet ) and  lowest=128 feet)\n",
      "\n",
      "Thought: The four states will have an elevation range of from 68.5 to 3906 meters\n",
      "\n",
      "Answer: Based on the information available the Elevation Range for these parts of North Dakota , South Dakota Wyoming and Montana is from approximately 64-3,907meters\n"
     ]
    }
   ],
   "source": [
    "# TODO: Talk about how prompt is an example of prompt engineering and assembling prompts on the fly\n",
    "\n",
    "\n",
    "prompt = f\"\"\"{few_shot_example}\n",
    "\n",
    "Question: {question}\n",
    "Thought:\"\"\"\n",
    "\n",
    "messages=[\n",
    "        {\"role\": \"system\", \"content\": \"Answer questions using a ReAct format: Thought → Action → Observation → Answer.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=messages,\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783e86f2-6d0b-4d01-9a61-c0cbe078eb20",
   "metadata": {},
   "source": [
    "# Chain of Thought (CoT) vs ReAct (Reasoning + Acting)\n",
    "\n",
    "## Chain of Thought (CoT)\n",
    "What it is:\n",
    "Just reasoning — step-by-step thoughts leading to an answer.\n",
    "\n",
    "\n",
    "|Strengths|Limitations|\n",
    "|:---|:---|\n",
    "|Good for pure reasoning tasks (math, logic, factual multi-step questions).|Doesn’t interact with external tools or sources.|\n",
    "|Easy to implement.|Limited when answers need fresh data, search, or database queries.|\n",
    "|Transparent: you can see the reasoning path.||\n",
    "\n",
    "\n",
    "## ReAct (Reasoning + Acting)\n",
    "What it is:\n",
    "A prompt style that combines reasoning (like CoT) with actions, such as calling tools, web searches, or internal functions. Often used with agents.\n",
    "\n",
    "\n",
    "|Strengths|Limitations|\n",
    "|---|---|\n",
    "|Perfect for agent workflows, e.g., answering based on tool output, RAG systems, browsing, calling APIs.|More complex to implement (you need tool handlers or agents).|\n",
    "|Enables decision-making with dynamic data.|Harder to debug if the chain gets too long or recursive.|\n",
    "|You can plug in your own tools, like databases, vector search, etc.||\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98161ecf-96dd-4007-98c6-0c886e85012e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Summary\n",
    "\n",
    "TODO: Create a summary of what we just saw\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
