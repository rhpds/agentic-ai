{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc2a0130-f86d-4ff1-8250-01850c957905",
   "metadata": {},
   "source": [
    "You should see that you are actually running this notebook from inside the venv which is a common \n",
    "practice and hear we can add dependencies etc and experiment safely with no consequesnced of interfering with our default Sustyem Python\n",
    "\n",
    "NOTE: Whilst python venvs are used extensively in development in Production we would effectively \"bake\" these into stronly versioned containers and run on OpenShift via GitOps - typically via helm charts and ArgoCD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c8da722-972e-4f6c-92a4-3403da0d44cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openai\n",
      "  Obtaining dependency information for openai from https://files.pythonhosted.org/packages/90/58/37ae3ca75936b824a0a5ca30491c968192007857319d6836764b548b9d9b/openai-1.77.0-py3-none-any.whl.metadata\n",
      "  Using cached openai-1.77.0-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting rich\n",
      "  Obtaining dependency information for rich from https://files.pythonhosted.org/packages/0d/9b/63f4c7ebc259242c89b3acafdb37b41d1185c07ff0011164674e9076b491/rich-14.0.0-py3-none-any.whl.metadata\n",
      "  Using cached rich-14.0.0-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /home/dev/venv/lib64/python3.12/site-packages (from openai) (4.9.0)\n",
      "Collecting distro<2,>=1.7.0 (from openai)\n",
      "  Obtaining dependency information for distro<2,>=1.7.0 from https://files.pythonhosted.org/packages/12/b3/231ffd4ab1fc9d679809f356cebee130ac7daa00d6d6f3206dd4fd137e9e/distro-1.9.0-py3-none-any.whl.metadata\n",
      "  Using cached distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /home/dev/venv/lib64/python3.12/site-packages (from openai) (0.28.1)\n",
      "Collecting jiter<1,>=0.4.0 (from openai)\n",
      "  Obtaining dependency information for jiter<1,>=0.4.0 from https://files.pythonhosted.org/packages/87/67/22728a86ef53589c3720225778f7c5fdb617080e3deaed58b04789418212/jiter-0.9.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Using cached jiter-0.9.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\n",
      "Collecting pydantic<3,>=1.9.0 (from openai)\n",
      "  Obtaining dependency information for pydantic<3,>=1.9.0 from https://files.pythonhosted.org/packages/e7/12/46b65f3534d099349e38ef6ec98b1a5a81f42536d17e0ba382c28c67ba67/pydantic-2.11.4-py3-none-any.whl.metadata\n",
      "  Using cached pydantic-2.11.4-py3-none-any.whl.metadata (66 kB)\n",
      "Requirement already satisfied: sniffio in /home/dev/venv/lib64/python3.12/site-packages (from openai) (1.3.1)\n",
      "Collecting tqdm>4 (from openai)\n",
      "  Obtaining dependency information for tqdm>4 from https://files.pythonhosted.org/packages/d0/30/dc54f88dd4a2b5dc8a0279bdd7270e735851848b762aeb1c1184ed1f6b14/tqdm-4.67.1-py3-none-any.whl.metadata\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /home/dev/venv/lib64/python3.12/site-packages (from openai) (4.13.2)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich)\n",
      "  Obtaining dependency information for markdown-it-py>=2.2.0 from https://files.pythonhosted.org/packages/42/d7/1ec15b46af6af88f19b8e5ffea08fa375d433c998b8a7639e76935c14f1f/markdown_it_py-3.0.0-py3-none-any.whl.metadata\n",
      "  Using cached markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/dev/venv/lib64/python3.12/site-packages (from rich) (2.19.1)\n",
      "Requirement already satisfied: idna>=2.8 in /home/dev/venv/lib64/python3.12/site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in /home/dev/venv/lib64/python3.12/site-packages (from httpx<1,>=0.23.0->openai) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in /home/dev/venv/lib64/python3.12/site-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /home/dev/venv/lib64/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich)\n",
      "  Obtaining dependency information for mdurl~=0.1 from https://files.pythonhosted.org/packages/b3/38/89ba8ad64ae25be8de66a6d463314cf1eb366222074cfda9ee839c56a4b4/mdurl-0.1.2-py3-none-any.whl.metadata\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3,>=1.9.0->openai)\n",
      "  Obtaining dependency information for annotated-types>=0.6.0 from https://files.pythonhosted.org/packages/78/b6/6307fbef88d9b5ee7421e68d78a9f162e0da4900bc5f5793f6d3d0e34fb8/annotated_types-0.7.0-py3-none-any.whl.metadata\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.33.2 (from pydantic<3,>=1.9.0->openai)\n",
      "  Obtaining dependency information for pydantic-core==2.33.2 from https://files.pythonhosted.org/packages/f9/41/4b043778cf9c4285d59742281a769eac371b9e47e35f98ad321349cc5d61/pydantic_core-2.33.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Using cached pydantic_core-2.33.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic<3,>=1.9.0->openai)\n",
      "  Obtaining dependency information for typing-inspection>=0.4.0 from https://files.pythonhosted.org/packages/31/08/aa4fdfb71f7de5176385bd9e90852eaf6b5d622735020ad600f2bab54385/typing_inspection-0.4.0-py3-none-any.whl.metadata\n",
      "  Using cached typing_inspection-0.4.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Using cached openai-1.77.0-py3-none-any.whl (662 kB)\n",
      "Using cached rich-14.0.0-py3-none-any.whl (243 kB)\n",
      "Using cached distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Using cached jiter-0.9.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (351 kB)\n",
      "Using cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Using cached pydantic-2.11.4-py3-none-any.whl (443 kB)\n",
      "Using cached pydantic_core-2.33.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Using cached typing_inspection-0.4.0-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: typing-inspection, tqdm, pydantic-core, mdurl, jiter, distro, annotated-types, pydantic, markdown-it-py, rich, openai\n",
      "Successfully installed annotated-types-0.7.0 distro-1.9.0 jiter-0.9.0 markdown-it-py-3.0.0 mdurl-0.1.2 openai-1.77.0 pydantic-2.11.4 pydantic-core-2.33.2 rich-14.0.0 tqdm-4.67.1 typing-inspection-0.4.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install openai rich"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "201a5506-5b38-4fcb-a0c6-45e67a8ed5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import re\n",
    "import httpx\n",
    "import os\n",
    "import rich\n",
    "import json\n",
    "from openai import OpenAI\n",
    "\n",
    "api_key = \"placeholder\"  # Can't be empty - otherwise will show an error In the interests of simplicity we will NOT use an api_key, in production on OpenShift AI etc there may be per user based keys\n",
    "# model = \"llama3.2:3b-instruct-fp16\"\n",
    "# model = \"phi4\"\n",
    "\n",
    "# Ollama settings for mistral-small\n",
    "# \n",
    "# vLLM settings\n",
    "model = \"mistralai/Mistral-Small-3.1-24B-Instruct-2503\"\n",
    "base_url = \"http://localhost:8000/v1/\" #openai/v1/\"\n",
    "\n",
    "# Llamastack Settings \n",
    "\n",
    "model = \"mistralai/Mistral-Small-3.1-24B-Instruct-2503\"\n",
    "model = \"microsoft/Phi-4-reasoning\"\n",
    "model = \"mistral-small:latest\"\n",
    "base_url = \"http://localhost:11434/v1/\" #openai/v1/\"\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url=base_url,\n",
    "    api_key=api_key,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7549175-8bc1-4d50-a5d9-f65cbfd6ac94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "11a8eb9c-fe97-4ebe-9c72-566bd0f3a975",
   "metadata": {},
   "source": [
    "# ReAct Agent Prompt with real actions and Agents\n",
    "\n",
    "We spoke that ReAct approach fits in well when some actions are involved. While we showed some examples of lookup actions, below, we get more real with actions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782dd43b-0161-4d16-b5d0-f0e463a18522",
   "metadata": {},
   "source": [
    "Once again, we are setting up a system prompt for ReACT.\n",
    "Notice, we talk about available actions here - which we did not do for CoTs as they were not applicable\n",
    "And also give examples - as we did for CoTs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "049bda6e-d2b2-4b99-9ab9-5471161a8736",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "You run in a loop of Thought, Action, PAUSE, Observation.\n",
    "At the end of the loop you output an Answer\n",
    "Use Thought to describe your thoughts about the question you have been asked.\n",
    "Use Action to run through one of the actions available to you - then return PAUSE.\n",
    "Observation will be the result of running those actions.\n",
    "\n",
    "Your available actions are:\n",
    "\n",
    "get_provision_status:\n",
    "e.g. get_provision_status: guid\n",
    "returns the status of a cloud deployment such as a virtual machine when gived a guid (globally unique identifier)\n",
    "\n",
    "log_error:\n",
    "e.g log_error: status\n",
    "When a guid has a provision_status ERROR call this with the return value of get_provision_status\n",
    "\n",
    "log_status:\n",
    "e.g log_status: status\n",
    "When a guid does not have an ERROR status call this with the return value of get_provision_status\n",
    "\n",
    "Example session:\n",
    "\n",
    "Question: What is the staus of cloud deployment with guid <guid>\n",
    "Thought: I should look up the status with get_provision_status \n",
    "Action: get_provision_status: guid \n",
    "PAUSE:\n",
    "\n",
    "You will be called again with this:\n",
    "\n",
    "Observation: Guid status \"SUCCESS: Completed\"\n",
    "\n",
    "You then call any necessary logging tools before outputing the status:\n",
    "\n",
    "Answer: Guid status \"SUCCESS: Completed\"\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6578f5-07f9-41cb-aed0-b6e50a15564b",
   "metadata": {},
   "source": [
    "### PAUSE\n",
    "\n",
    "It is very important.\n",
    "- After an Action (e.g., lookup, calculation, API call), the model stops (pause) to wait for the result.\n",
    "- It does not hallucinate the result.\n",
    "- It needs real information from the environment (tools, web, database, code, etc.).\n",
    "- Only after the result (Observation) comes back does it continue reasoning.\n",
    "- Without the pause, the model would guess the observation — which can lead to wrong or invented answers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76eb490-6a3d-4c8c-b36c-0c0de31380e6",
   "metadata": {},
   "source": [
    "Here is a simple agent definition\n",
    "Note - the roles: system, user, assistant\n",
    "\n",
    "In the OpenAI SDK, when you're calling models like gpt-4, gpt-4o, or gpt-3.5-turbo using the chat/completions endpoint, the roles used are:\n",
    "\n",
    "|Role | Purpose|\n",
    "|---|---|\n",
    "|system | Sets the behavior, identity, style, or rules of the assistant. (e.g., \"You are a helpful assistant.\")|\n",
    "|user | Represents input/questions from the human user.|\n",
    "|assistant | Represents the model's previous responses.|\n",
    "\n",
    "\n",
    "- system: Optional, but powerful. Sets context at the start. You usually have one.\n",
    "- user: Each time a person talks, you add a user message.\n",
    "- assistant: Each time the model replies, its response is logged as an assistant message.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c782d6-ce12-4bdb-8086-af35c30ce378",
   "metadata": {},
   "source": [
    "Next we define am Agent in raw Python, this module the only AI related framework we use is the open AI API simply to send messages to and from the LLM\n",
    "\n",
    "In production there a huge number of genetic frameworks for python, JavaScript, type script, other languages, and and enterprises would typically select one or more frameworks that map onto their needs. \n",
    "Well, noon, Payson frameworks include:\n",
    "\n",
    "* OpenAI Agent SDK\n",
    "* CrewAI\n",
    "* AutoGen (Microsoft)\n",
    "* LangGraph (from LangChain)\n",
    "* PydanticAI\n",
    "* SmolAgents\n",
    "\n",
    "There is however tremendous value in writing this framework in pure Python as it exposes the simplicity in creating an Agent with a ReAct Prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8fff260b-2ee5-4ae9-b66b-4c313d5df9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, system=\"\"):\n",
    "        self.system = system\n",
    "        self.messages = []\n",
    "        if self.system:\n",
    "            self.messages.append({\"role\": \"system\", \"content\": system})\n",
    "\n",
    "    def __call__(self, message):\n",
    "        self.messages.append({\"role\": \"user\", \"content\": message})\n",
    "        result = self.execute()\n",
    "        self.messages.append({\"role\": \"assistant\", \"content\": result})\n",
    "        return result\n",
    "\n",
    "    def execute(self):\n",
    "        completion = client.chat.completions.create(\n",
    "            model=model,\n",
    "            temperature=0,\n",
    "            messages=self.messages)\n",
    "        return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8af50d-1c7e-4cab-93de-01d7a0942cb4",
   "metadata": {},
   "source": [
    "Next, we define the actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65f2cf93-87b3-430e-9a4b-0bec9eb9e004",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "'''\n",
    "First of the *fake* functions to test if the LLM/Prompt will ReAct correctly\n",
    "taking different paths on different results\n",
    "'''\n",
    "\n",
    "def get_provision_status(guid):\n",
    "\n",
    "    #// call to MCP Server AAP2 Controller\n",
    "    # // foo = bar()\n",
    "    \n",
    "    status_messages = [\n",
    "        \"INFO: Initializing\",\n",
    "        \"INFO: In progress\",\n",
    "        \"ERROR: Failed\",\n",
    "        \"ERROR: API Timeout\",\n",
    "        \"ERROR: Rate Limited\",\n",
    "        \"WARNING: Minor errors\",\n",
    "        \"SUCCESS: Completed\"\n",
    "    ]\n",
    "        # \"INFO: Finalizing\",\n",
    "    # return random.choice(f\"{guid} status: {status_messages}\")\n",
    "    status = random.choice(status_messages)\n",
    "    return f\"{guid} status: {status}\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6dc6f2d9-80c9-4ad6-9165-d0158c049954",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_error(status):\n",
    "    print(f\"{status} Logged status to Slack.\")\n",
    "    print(f\"{status} Opened Jira Ticket with Status.\")\n",
    "    return 0\n",
    "\n",
    "def log_status(status):\n",
    "    print(f\"{status} Logged status to Slack.\")\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1d41d31-7186-474d-b405-9a5396221d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "known_actions = {\n",
    "   \"log_status\": log_status,\n",
    "   \"log_error\": log_error,\n",
    "   \"get_provision_status\": get_provision_status,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef24f68d-edf5-4e74-8d87-4fb509133082",
   "metadata": {},
   "source": [
    "Now we call the agent with to set up the system prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d47df89b-5030-4e6a-a3bd-d16e72655bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "abot = Agent(prompt)\n",
    "# Run the cell below with this line commented. \n",
    "# And then once again run it with this line uncommented\n",
    "# See where the flow stops ?\n",
    "\n",
    "#abot(\"I have a deployments running with guid: 1adr4 what is its status\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df37b053-6f9e-42ef-aa18-915bd47d6f14",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: App -> LLM:\n",
      "\n",
      "Role: system\n",
      "Content:\n",
      "\n",
      "You run in a loop of Thought, Action, PAUSE, Observation.\n",
      "At the end of the loop you output an Answer\n",
      "Use Thought to describe your thoughts about the question you have been asked.\n",
      "Use Action to run through one of the actions available to you - then return PAUSE.\n",
      "Observation will be the result of running those actions.\n",
      "\n",
      "Your available actions are:\n",
      "\n",
      "get_provision_status:\n",
      "e.g. get_provision_status: guid\n",
      "returns the status of a cloud deployment such as a virtual machine when gived a guid (globally unique identifier)\n",
      "\n",
      "log_error:\n",
      "e.g log_error: status\n",
      "When a guid has a provision_status ERROR call this with the return value of get_provision_status\n",
      "\n",
      "log_status:\n",
      "e.g log_status: status\n",
      "When a guid does not have an ERROR status call this with the return value of get_provision_status\n",
      "\n",
      "Example session:\n",
      "\n",
      "Question: What is the staus of cloud deployment with guid <guid>\n",
      "Thought: I should look up the status with get_provision_status \n",
      "Action: get_provision_status: guid \n",
      "PAUSE:\n",
      "\n",
      "You will be called again with this:\n",
      "\n",
      "Observation: Guid status \"SUCCESS: Completed\"\n",
      "\n",
      "You then call any necessary logging tools before outputing the status:\n",
      "\n",
      "Answer: Guid status \"SUCCESS: Completed\"\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, message in  enumerate(abot.messages):\n",
    "    if message[\"role\"] != \"assistant\":\n",
    "        print(f\"Step {i}: App -> LLM:\\n\")\n",
    "    else:\n",
    "        print(f\"Step {i}: App <- LLM:\\n\")\n",
    "    print(f\"Role: {message['role']}\\nContent:\\n\\n{message['content']}\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57606abf-1465-4c3d-a8fa-787d64da0826",
   "metadata": {},
   "source": [
    "## Now Let's automate all this"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f3c1c5cb-e22b-4902-81e0-933e0fa9e4f3",
   "metadata": {},
   "source": [
    "action_re = re.compile(\"^Action: (\\w+): (.*)$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d3239a57-b16b-40f8-837c-4b53f866a255",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_re = re.compile(r'^Action: (\\w+): (.*)$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d6a22524-fb84-4803-a9e3-c9504378d42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query(question, max_turns=5):\n",
    "    i = 0\n",
    "    bot = Agent(prompt)\n",
    "    next_prompt = question\n",
    "    print(\"Step 0\")\n",
    "    while i < max_turns:\n",
    "        i += 1\n",
    "        result = bot(next_prompt)\n",
    "        print(result)\n",
    "        actions = [\n",
    "            action_re.match(a)\n",
    "            for a in result.split('\\n')\n",
    "            if action_re.match(a)\n",
    "        ]\n",
    "        if actions:\n",
    "            print(f\"\\nStep {i}\")\n",
    "            # print(f\"Actions:\\n\\n{actions}\")\n",
    "            action, action_inputs = actions[0].groups()\n",
    "            if action not in known_actions:\n",
    "                raise Exception(f\"Unknown action: {action}: -- running {action} {action_inputs}\")\n",
    "            observation = known_actions[action](action_inputs)\n",
    "            print(f\"Observation: {observation}\")\n",
    "            next_prompt = f\"Observation: {observation}\"\n",
    "        else:\n",
    "            return\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "98d9646e-eea8-41bb-a2ea-fad299ffebff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0\n",
      "Thought: I should look up the status of the cloud deployment with get_provision_status\n",
      "\n",
      "Action: get_provision_status: 1adr4\n",
      "\n",
      "PAUSE\n",
      "\n",
      "Step 1\n",
      "Observation: 1adr4 status: WARNING: Minor errors\n",
      "Thought: The status indicates a warning. I need to log this status.\n",
      "\n",
      "Action: log_status: WARNING: Minor errors\n",
      "\n",
      "PAUSE\n",
      "\n",
      "Step 2\n",
      "WARNING: Minor errors Logged status to Slack.\n",
      "Observation: 0\n",
      "Answer: Guid status \"WARNING: Minor errors\"\n"
     ]
    }
   ],
   "source": [
    "question = \"\"\"\n",
    "I have a deployment running with guid: 1adr4, what is its status\n",
    "\"\"\"\n",
    "\n",
    "query(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6bb3bd2e-d259-4351-a7a0-4a57c2cbd5e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0\n",
      "Thought: I need to check the provision status of each guid using get_provision_status. Then based on the returned status I will call either log_error or log_status.\n",
      "Action: get_provision_status: 1adr4\n",
      "PAUSE\n",
      "\n",
      "Step 1\n",
      "Observation: 1adr4 status: WARNING: Minor errors\n",
      "Thought: The status is a warning so I should log it with log_status\n",
      "Action: log_status: WARNING: Minor errors\n",
      "PAUSE\n",
      "\n",
      "Step 2\n",
      "WARNING: Minor errors Logged status to Slack.\n",
      "Observation: 0\n",
      "Thought: Now I need to check the provision status of guid aabf5.\n",
      "Action: get_provision_status: aabf5\n",
      "PAUSE\n",
      "\n",
      "Step 3\n",
      "Observation: aabf5 status: INFO: In progress\n",
      "Thought: The status is info so I should log it with log_status\n",
      "Action: log_status: INFO: In progress\n",
      "PAUSE\n",
      "\n",
      "Step 4\n",
      "INFO: In progress Logged status to Slack.\n",
      "Observation: 0\n",
      "Thought: Now I need to check the provision status of guid 45663.\n",
      "Action: get_provision_status: 45663\n",
      "PAUSE\n",
      "\n",
      "Step 5\n",
      "Observation: 45663 status: ERROR: Failed\n",
      "Thought: The status is an error so I should log it with log_error\n",
      "Action: log_error: ERROR: Failed\n",
      "PAUSE\n",
      "\n",
      "Step 6\n",
      "ERROR: Failed Logged status to Slack.\n",
      "ERROR: Failed Opened Jira Ticket with Status.\n",
      "Observation: 0\n",
      "Thought: Now I need to check the provision status of guid 45ghb.\n",
      "Action: get_provision_status: 45ghb\n",
      "PAUSE\n",
      "\n",
      "Step 7\n",
      "Observation: 45ghb status: ERROR: API Timeout\n",
      "Thought: The status is an error so I should log it with log_error\n",
      "Action: log_error: ERROR: API Timeout\n",
      "PAUSE\n",
      "\n",
      "Step 8\n",
      "ERROR: API Timeout Logged status to Slack.\n",
      "ERROR: API Timeout Opened Jira Ticket with Status.\n",
      "Observation: 0\n",
      "Answer:\n",
      "```json\n",
      "{\n",
      "    \"deployments\": [\n",
      "        {\n",
      "            \"guid\": \"1adr4\",\n",
      "            \"status\": \"WARNING: Minor errors\",\n",
      "            \"logging_service\": \"log_status\"\n",
      "        },\n",
      "        {\n",
      "            \"guid\": \"aabf5\",\n",
      "            \"status\": \"INFO: In progress\",\n",
      "            \"logging_service\": \"log_status\"\n",
      "        },\n",
      "        {\n",
      "            \"guid\": \"45663\",\n",
      "            \"status\": \"ERROR: Failed\",\n",
      "            \"logging_service\": \"log_error\"\n",
      "        },\n",
      "        {\n",
      "            \"guid\": \"45ghb\",\n",
      "            \"status\": \"ERROR: API Timeout\",\n",
      "            \"logging_service\": \"log_error\"\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "# worked ok with qwen3:32b\n",
    "\n",
    "# question = \"\"\"\n",
    "# I have deployments running with guids: 1adr4, aabf5, 45663, and 45ghb\n",
    "# For each deployment get their provision status\n",
    "# If they have an error use the log_error tool\n",
    "# If the were successful use the log_message tool\n",
    "# Once finished with all deployments output their guids, status, and logging services:\n",
    "\n",
    "# * In a simple table\n",
    "# * As JSON \n",
    "# \"\"\"\n",
    "\n",
    "question = \"\"\"\n",
    "I have deployments running with guids: 1adr4, aabf5, 45663, and 45ghb\n",
    "For each deployment get their provision status then log their status with the appropriate tool\n",
    "Once finished with all deployments output their guids, status, and logging services as JSON\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "query(question, max_turns=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5caf3a34-156a-408b-ae9a-e47eaafc1005",
   "metadata": {},
   "source": [
    "# Reasoning\n",
    "Capabilities are improving every day. The o3-mini accepts a reasoning_effort parameter and can reason without sophisticated prompt injections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d04ad974-c477-4338-a068-abbb6a4a5ebe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-856', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='You can use the following bash script to achieve this:\\n\\n```bash\\n#!/bin/bash\\n\\n# Input string for the matrix\\nmatrix_str=$1\\n\\n# Split the input string into rows\\nIFS=\\',\\' read -ra rows <<< \"$matrix_str\"\\n\\n# Initialize an empty array to hold the transposed matrix\\ntranspose_matrix=()\\n\\n# Iterate over each column in the columns of the given matrix\\nfor ((i=0; i<${#rows[@]}; i++)); do\\n  # Split the columns into elements\\n  col=(${rows[i]})\\n  \\n  # If it\\'s not the first row, add its previous row\\n  if [ $i -gt 0 ]; then\\n    prev_row=${rows[i-1]}\\n    \\n    index=$(echo \"$prev_row\" | cut -d\\',\\' -f$((i+1)))\\n    second_row=(${col[$index]})\\n    \\n    for ((j=0; j<${#rows[@]}; j++)); do\\n      if [ $j -ne $(($i + 1)) ]; then\\n        transpose_matrix+=(\"${second_row[$j]},${cols[j]}\")\\n      fi\\n    done\\n  \\n  else\\n    # The first row becomes the first column of the transposed matrix\\n    for ((j=0; j<${#rows[@]}; j++)); do\\n      col_str=${col[$j]}\\n      for k in ${!secondrows}; do\\n        index=$k\\n        transposematrix+=($colstr,${secondrows[k]})\\n      done\\n    done\\n    \\n  fi\\n\\n done\\n\\n  # Print the transpose matrix\\necho \"${transpose_matrix[@]}\"\\n```\\n\\nNote that this script assumes a square matrix (i.e., has the same number of rows and columns). \\n\\nHow you would use it:\\n\\n```shell\\n./matrix_transpose.sh \"[1,2],[3,4],[5,6]\"\\n```\\nYou can modify it to have more features like error checking.', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=None))], created=1746496013, model='llama3.2:3b-instruct-fp16', object='chat.completion', service_tier=None, system_fingerprint='fp_ollama', usage=CompletionUsage(completion_tokens=408, prompt_tokens=62, total_tokens=470, completion_tokens_details=None, prompt_tokens_details=None))\n",
      "-----------------\n",
      "You can use the following bash script to achieve this:\n",
      "\n",
      "```bash\n",
      "#!/bin/bash\n",
      "\n",
      "# Input string for the matrix\n",
      "matrix_str=$1\n",
      "\n",
      "# Split the input string into rows\n",
      "IFS=',' read -ra rows <<< \"$matrix_str\"\n",
      "\n",
      "# Initialize an empty array to hold the transposed matrix\n",
      "transpose_matrix=()\n",
      "\n",
      "# Iterate over each column in the columns of the given matrix\n",
      "for ((i=0; i<${#rows[@]}; i++)); do\n",
      "  # Split the columns into elements\n",
      "  col=(${rows[i]})\n",
      "  \n",
      "  # If it's not the first row, add its previous row\n",
      "  if [ $i -gt 0 ]; then\n",
      "    prev_row=${rows[i-1]}\n",
      "    \n",
      "    index=$(echo \"$prev_row\" | cut -d',' -f$((i+1)))\n",
      "    second_row=(${col[$index]})\n",
      "    \n",
      "    for ((j=0; j<${#rows[@]}; j++)); do\n",
      "      if [ $j -ne $(($i + 1)) ]; then\n",
      "        transpose_matrix+=(\"${second_row[$j]},${cols[j]}\")\n",
      "      fi\n",
      "    done\n",
      "  \n",
      "  else\n",
      "    # The first row becomes the first column of the transposed matrix\n",
      "    for ((j=0; j<${#rows[@]}; j++)); do\n",
      "      col_str=${col[$j]}\n",
      "      for k in ${!secondrows}; do\n",
      "        index=$k\n",
      "        transposematrix+=($colstr,${secondrows[k]})\n",
      "      done\n",
      "    done\n",
      "    \n",
      "  fi\n",
      "\n",
      " done\n",
      "\n",
      "  # Print the transpose matrix\n",
      "echo \"${transpose_matrix[@]}\"\n",
      "```\n",
      "\n",
      "Note that this script assumes a square matrix (i.e., has the same number of rows and columns). \n",
      "\n",
      "How you would use it:\n",
      "\n",
      "```shell\n",
      "./matrix_transpose.sh \"[1,2],[3,4],[5,6]\"\n",
      "```\n",
      "You can modify it to have more features like error checking.\n"
     ]
    }
   ],
   "source": [
    "# model = \"o3-mini\"\n",
    "prompt = \"\"\"\n",
    "Write a bash script that takes a matrix represented as a string with \n",
    "format '[1,2],[3,4],[5,6]' and prints the transpose in the same format.\n",
    "\"\"\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model = model,\n",
    "    reasoning_effort=\"medium\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\", \n",
    "            \"content\": prompt\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response)\n",
    "print('-----------------')\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78716910-84ca-42b6-9f2b-faa39f4df1fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af670db-a11b-4a53-8237-d7ee3e1ba64d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
