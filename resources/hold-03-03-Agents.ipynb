{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8987006-614e-41b7-9c2f-aa6359e8e231",
   "metadata": {},
   "source": [
    "# Tools, API & Microservices\n",
    "\n",
    "Now that we have seen the power of prompts and a look how they come together in a simple agent, lets explore formally a few other concepts.\n",
    "\n",
    "1. Function calling\n",
    "2. Tool Calling\n",
    "3. Introduction to Agents\n",
    "4. Agents calling tools\n",
    "5. Agentic Patterns\n",
    "6. Agents and Microservices\n",
    "\n",
    "_Each module is typically dependent on the prior modules having been completed successfully_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8e6d718c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai-agents==0.0.13 in /Users/tok/Dropbox/PARAL/Resources/virtual-envs/summit-agentic-ai/lib/python3.12/site-packages (0.0.13)\n",
      "Requirement already satisfied: griffe<2,>=1.5.6 in /Users/tok/Dropbox/PARAL/Resources/virtual-envs/summit-agentic-ai/lib/python3.12/site-packages (from openai-agents==0.0.13) (1.7.3)\n",
      "Requirement already satisfied: mcp<2,>=1.6.0 in /Users/tok/Dropbox/PARAL/Resources/virtual-envs/summit-agentic-ai/lib/python3.12/site-packages (from openai-agents==0.0.13) (1.7.1)\n",
      "Requirement already satisfied: openai>=1.76.0 in /Users/tok/Dropbox/PARAL/Resources/virtual-envs/summit-agentic-ai/lib/python3.12/site-packages (from openai-agents==0.0.13) (1.76.2)\n",
      "Requirement already satisfied: pydantic<3,>=2.10 in /Users/tok/Dropbox/PARAL/Resources/virtual-envs/summit-agentic-ai/lib/python3.12/site-packages (from openai-agents==0.0.13) (2.11.4)\n",
      "Requirement already satisfied: requests<3,>=2.0 in /Users/tok/Dropbox/PARAL/Resources/virtual-envs/summit-agentic-ai/lib/python3.12/site-packages (from openai-agents==0.0.13) (2.32.3)\n",
      "Requirement already satisfied: types-requests<3,>=2.0 in /Users/tok/Dropbox/PARAL/Resources/virtual-envs/summit-agentic-ai/lib/python3.12/site-packages (from openai-agents==0.0.13) (2.32.0.20250328)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.12.2 in /Users/tok/Dropbox/PARAL/Resources/virtual-envs/summit-agentic-ai/lib/python3.12/site-packages (from openai-agents==0.0.13) (4.13.2)\n",
      "Requirement already satisfied: colorama>=0.4 in /Users/tok/Dropbox/PARAL/Resources/virtual-envs/summit-agentic-ai/lib/python3.12/site-packages (from griffe<2,>=1.5.6->openai-agents==0.0.13) (0.4.6)\n",
      "Requirement already satisfied: anyio>=4.5 in /Users/tok/Dropbox/PARAL/Resources/virtual-envs/summit-agentic-ai/lib/python3.12/site-packages (from mcp<2,>=1.6.0->openai-agents==0.0.13) (4.9.0)\n",
      "Requirement already satisfied: httpx-sse>=0.4 in /Users/tok/Dropbox/PARAL/Resources/virtual-envs/summit-agentic-ai/lib/python3.12/site-packages (from mcp<2,>=1.6.0->openai-agents==0.0.13) (0.4.0)\n",
      "Requirement already satisfied: httpx>=0.27 in /Users/tok/Dropbox/PARAL/Resources/virtual-envs/summit-agentic-ai/lib/python3.12/site-packages (from mcp<2,>=1.6.0->openai-agents==0.0.13) (0.28.1)\n",
      "Requirement already satisfied: pydantic-settings>=2.5.2 in /Users/tok/Dropbox/PARAL/Resources/virtual-envs/summit-agentic-ai/lib/python3.12/site-packages (from mcp<2,>=1.6.0->openai-agents==0.0.13) (2.9.1)\n",
      "Requirement already satisfied: python-multipart>=0.0.9 in /Users/tok/Dropbox/PARAL/Resources/virtual-envs/summit-agentic-ai/lib/python3.12/site-packages (from mcp<2,>=1.6.0->openai-agents==0.0.13) (0.0.20)\n",
      "Requirement already satisfied: sse-starlette>=1.6.1 in /Users/tok/Dropbox/PARAL/Resources/virtual-envs/summit-agentic-ai/lib/python3.12/site-packages (from mcp<2,>=1.6.0->openai-agents==0.0.13) (2.3.4)\n",
      "Requirement already satisfied: starlette>=0.27 in /Users/tok/Dropbox/PARAL/Resources/virtual-envs/summit-agentic-ai/lib/python3.12/site-packages (from mcp<2,>=1.6.0->openai-agents==0.0.13) (0.46.2)\n",
      "Requirement already satisfied: uvicorn>=0.23.1 in /Users/tok/Dropbox/PARAL/Resources/virtual-envs/summit-agentic-ai/lib/python3.12/site-packages (from mcp<2,>=1.6.0->openai-agents==0.0.13) (0.34.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/tok/Dropbox/PARAL/Resources/virtual-envs/summit-agentic-ai/lib/python3.12/site-packages (from pydantic<3,>=2.10->openai-agents==0.0.13) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /Users/tok/Dropbox/PARAL/Resources/virtual-envs/summit-agentic-ai/lib/python3.12/site-packages (from pydantic<3,>=2.10->openai-agents==0.0.13) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /Users/tok/Dropbox/PARAL/Resources/virtual-envs/summit-agentic-ai/lib/python3.12/site-packages (from pydantic<3,>=2.10->openai-agents==0.0.13) (0.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/tok/Dropbox/PARAL/Resources/virtual-envs/summit-agentic-ai/lib/python3.12/site-packages (from requests<3,>=2.0->openai-agents==0.0.13) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/tok/Dropbox/PARAL/Resources/virtual-envs/summit-agentic-ai/lib/python3.12/site-packages (from requests<3,>=2.0->openai-agents==0.0.13) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/tok/Dropbox/PARAL/Resources/virtual-envs/summit-agentic-ai/lib/python3.12/site-packages (from requests<3,>=2.0->openai-agents==0.0.13) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/tok/Dropbox/PARAL/Resources/virtual-envs/summit-agentic-ai/lib/python3.12/site-packages (from requests<3,>=2.0->openai-agents==0.0.13) (2025.4.26)\n",
      "Requirement already satisfied: sniffio>=1.1 in /Users/tok/Dropbox/PARAL/Resources/virtual-envs/summit-agentic-ai/lib/python3.12/site-packages (from anyio>=4.5->mcp<2,>=1.6.0->openai-agents==0.0.13) (1.3.1)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/tok/Dropbox/PARAL/Resources/virtual-envs/summit-agentic-ai/lib/python3.12/site-packages (from httpx>=0.27->mcp<2,>=1.6.0->openai-agents==0.0.13) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /Users/tok/Dropbox/PARAL/Resources/virtual-envs/summit-agentic-ai/lib/python3.12/site-packages (from httpcore==1.*->httpx>=0.27->mcp<2,>=1.6.0->openai-agents==0.0.13) (0.16.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/tok/Dropbox/PARAL/Resources/virtual-envs/summit-agentic-ai/lib/python3.12/site-packages (from openai>=1.76.0->openai-agents==0.0.13) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /Users/tok/Dropbox/PARAL/Resources/virtual-envs/summit-agentic-ai/lib/python3.12/site-packages (from openai>=1.76.0->openai-agents==0.0.13) (0.9.0)\n",
      "Requirement already satisfied: tqdm>4 in /Users/tok/Dropbox/PARAL/Resources/virtual-envs/summit-agentic-ai/lib/python3.12/site-packages (from openai>=1.76.0->openai-agents==0.0.13) (4.67.1)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in /Users/tok/Dropbox/PARAL/Resources/virtual-envs/summit-agentic-ai/lib/python3.12/site-packages (from pydantic-settings>=2.5.2->mcp<2,>=1.6.0->openai-agents==0.0.13) (1.1.0)\n",
      "Requirement already satisfied: click>=7.0 in /Users/tok/Dropbox/PARAL/Resources/virtual-envs/summit-agentic-ai/lib/python3.12/site-packages (from uvicorn>=0.23.1->mcp<2,>=1.6.0->openai-agents==0.0.13) (8.1.8)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install \"openai-agents==0.0.13\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201a5506-5b38-4fcb-a0c6-45e67a8ed5c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports complete, Client initialized\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import re\n",
    "import httpx\n",
    "import os\n",
    "import rich\n",
    "import json\n",
    "from openai import OpenAI\n",
    "\n",
    "api_key = \"placeholder\"  \n",
    "model = \"mistral-small:latest\" # Other options here include \"qwen3:32b\"\n",
    "\n",
    "\n",
    "base_url = \"http://localhost:11434/v1/\" #openai/v1/\"\n",
    "# model = \"qwen3:32b\"\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url=base_url,\n",
    "    api_key=api_key,\n",
    ")\n",
    "\n",
    "print(\"Imports complete, Client initialized\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b841d14-dc51-46a4-8bee-cfcecc4b122e",
   "metadata": {},
   "source": [
    "# Agent\n",
    "\n",
    "Agents are the heart of complex AI applications. They combine inference, memory, safety, and tool usage into coherent workflows. At its core, an agent follows a sophisticated execution loop that enables multi-step reasoning, tool usage, and safety checks.\n",
    "Ref: https://llama-stack.readthedocs.io/en/latest/building_applications/agent_execution_loop.html\n",
    "\n",
    "An Agentic-AI eco system is much larger than the LLM/s which it uses. While LLMs are being used, the agentic structure helps us to automate using those sophisticated prompts that we talked about.  The power of agentic AI is not only in the model, but in the orchestration—how you structure the workflow to get durable, repeatable outcomes without hand-holding.\n",
    "\n",
    "\n",
    "### Goal-Oriented Looping\n",
    "\n",
    "- A raw LLM gives one-shot answers. An agent keeps trying, planning multiple steps, checking for errors, adapting.\n",
    "- Think of it as: “Try → Check → Revise → Retry → Finish” or \"Thought → Action → Observation → Repeat → Answer\"\n",
    "- The loop itself enforces discipline and depth.\n",
    "- Without that structure, the LLM might shortcut the process.\n",
    "\n",
    "### Memory & Scratchpad\n",
    "\n",
    "Agents can keep track of:\n",
    "- What they’ve tried\n",
    "- What the intermediate results were\n",
    "- What the user originally wanted\n",
    "- LLM alone doesn’t track history or outcomes unless explicitly given.\n",
    "\n",
    "### Tool Use\n",
    "\n",
    "- Agents can call APIs, browse docs, or query databases. LLM alone hallucinates data. An agent says: “I don’t know—let me look it up.”\n",
    "\n",
    "### Decomposition\n",
    "\n",
    "- Agents break big tasks into smaller ones.\n",
    "- LLMs can do this, but often need a prompt to do so.\n",
    "- Agents automate that “thinking out loud.”\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d35782-20e8-430b-9b5d-e2fdc63cb5f6",
   "metadata": {},
   "source": [
    "## Agent calling tools\n",
    "1. Simply demonstrates an agent using a tool.\n",
    "1. Look at the brevity of the code compared to doing a function calling all on our own.\n",
    "1. Play with the question that can be asked to agent to see how it can handle questions that may or may not require the tool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36a8bee-781e-4f1f-a749-1fd52bbfefb7",
   "metadata": {},
   "source": [
    "First we will need to insatll the Python `agents` dependancy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bd53c2a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai-agents==0.0.13 in /Users/tok/Dropbox/PARAL/Resources/virtual-envs/summit-agentic-ai/lib/python3.12/site-packages (0.0.13)\n",
      "Requirement already satisfied: griffe<2,>=1.5.6 in /Users/tok/Dropbox/PARAL/Resources/virtual-envs/summit-agentic-ai/lib/python3.12/site-packages (from openai-agents==0.0.13) (1.7.3)\n",
      "Requirement already satisfied: mcp<2,>=1.6.0 in /Users/tok/Dropbox/PARAL/Resources/virtual-envs/summit-agentic-ai/lib/python3.12/site-packages (from openai-agents==0.0.13) (1.7.1)\n",
      "Requirement already satisfied: openai>=1.76.0 in /Users/tok/Dropbox/PARAL/Resources/virtual-envs/summit-agentic-ai/lib/python3.12/site-packages (from openai-agents==0.0.13) (1.76.2)\n",
      "Requirement already satisfied: pydantic<3,>=2.10 in /Users/tok/Dropbox/PARAL/Resources/virtual-envs/summit-agentic-ai/lib/python3.12/site-packages (from openai-agents==0.0.13) (2.11.4)\n",
      "Requirement already satisfied: requests<3,>=2.0 in /Users/tok/Dropbox/PARAL/Resources/virtual-envs/summit-agentic-ai/lib/python3.12/site-packages (from openai-agents==0.0.13) (2.32.3)\n",
      "Requirement already satisfied: types-requests<3,>=2.0 in /Users/tok/Dropbox/PARAL/Resources/virtual-envs/summit-agentic-ai/lib/python3.12/site-packages (from openai-agents==0.0.13) (2.32.0.20250328)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.12.2 in /Users/tok/Dropbox/PARAL/Resources/virtual-envs/summit-agentic-ai/lib/python3.12/site-packages (from openai-agents==0.0.13) (4.13.2)\n",
      "Requirement already satisfied: colorama>=0.4 in /Users/tok/Dropbox/PARAL/Resources/virtual-envs/summit-agentic-ai/lib/python3.12/site-packages (from griffe<2,>=1.5.6->openai-agents==0.0.13) (0.4.6)\n",
      "Requirement already satisfied: anyio>=4.5 in /Users/tok/Dropbox/PARAL/Resources/virtual-envs/summit-agentic-ai/lib/python3.12/site-packages (from mcp<2,>=1.6.0->openai-agents==0.0.13) (4.9.0)\n",
      "Requirement already satisfied: httpx-sse>=0.4 in /Users/tok/Dropbox/PARAL/Resources/virtual-envs/summit-agentic-ai/lib/python3.12/site-packages (from mcp<2,>=1.6.0->openai-agents==0.0.13) (0.4.0)\n",
      "Requirement already satisfied: httpx>=0.27 in /Users/tok/Dropbox/PARAL/Resources/virtual-envs/summit-agentic-ai/lib/python3.12/site-packages (from mcp<2,>=1.6.0->openai-agents==0.0.13) (0.28.1)\n",
      "Requirement already satisfied: pydantic-settings>=2.5.2 in /Users/tok/Dropbox/PARAL/Resources/virtual-envs/summit-agentic-ai/lib/python3.12/site-packages (from mcp<2,>=1.6.0->openai-agents==0.0.13) (2.9.1)\n",
      "Requirement already satisfied: python-multipart>=0.0.9 in /Users/tok/Dropbox/PARAL/Resources/virtual-envs/summit-agentic-ai/lib/python3.12/site-packages (from mcp<2,>=1.6.0->openai-agents==0.0.13) (0.0.20)\n",
      "Requirement already satisfied: sse-starlette>=1.6.1 in /Users/tok/Dropbox/PARAL/Resources/virtual-envs/summit-agentic-ai/lib/python3.12/site-packages (from mcp<2,>=1.6.0->openai-agents==0.0.13) (2.3.4)\n",
      "Requirement already satisfied: starlette>=0.27 in /Users/tok/Dropbox/PARAL/Resources/virtual-envs/summit-agentic-ai/lib/python3.12/site-packages (from mcp<2,>=1.6.0->openai-agents==0.0.13) (0.46.2)\n",
      "Requirement already satisfied: uvicorn>=0.23.1 in /Users/tok/Dropbox/PARAL/Resources/virtual-envs/summit-agentic-ai/lib/python3.12/site-packages (from mcp<2,>=1.6.0->openai-agents==0.0.13) (0.34.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/tok/Dropbox/PARAL/Resources/virtual-envs/summit-agentic-ai/lib/python3.12/site-packages (from pydantic<3,>=2.10->openai-agents==0.0.13) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /Users/tok/Dropbox/PARAL/Resources/virtual-envs/summit-agentic-ai/lib/python3.12/site-packages (from pydantic<3,>=2.10->openai-agents==0.0.13) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /Users/tok/Dropbox/PARAL/Resources/virtual-envs/summit-agentic-ai/lib/python3.12/site-packages (from pydantic<3,>=2.10->openai-agents==0.0.13) (0.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/tok/Dropbox/PARAL/Resources/virtual-envs/summit-agentic-ai/lib/python3.12/site-packages (from requests<3,>=2.0->openai-agents==0.0.13) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/tok/Dropbox/PARAL/Resources/virtual-envs/summit-agentic-ai/lib/python3.12/site-packages (from requests<3,>=2.0->openai-agents==0.0.13) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/tok/Dropbox/PARAL/Resources/virtual-envs/summit-agentic-ai/lib/python3.12/site-packages (from requests<3,>=2.0->openai-agents==0.0.13) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/tok/Dropbox/PARAL/Resources/virtual-envs/summit-agentic-ai/lib/python3.12/site-packages (from requests<3,>=2.0->openai-agents==0.0.13) (2025.4.26)\n",
      "Requirement already satisfied: sniffio>=1.1 in /Users/tok/Dropbox/PARAL/Resources/virtual-envs/summit-agentic-ai/lib/python3.12/site-packages (from anyio>=4.5->mcp<2,>=1.6.0->openai-agents==0.0.13) (1.3.1)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/tok/Dropbox/PARAL/Resources/virtual-envs/summit-agentic-ai/lib/python3.12/site-packages (from httpx>=0.27->mcp<2,>=1.6.0->openai-agents==0.0.13) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /Users/tok/Dropbox/PARAL/Resources/virtual-envs/summit-agentic-ai/lib/python3.12/site-packages (from httpcore==1.*->httpx>=0.27->mcp<2,>=1.6.0->openai-agents==0.0.13) (0.16.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/tok/Dropbox/PARAL/Resources/virtual-envs/summit-agentic-ai/lib/python3.12/site-packages (from openai>=1.76.0->openai-agents==0.0.13) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /Users/tok/Dropbox/PARAL/Resources/virtual-envs/summit-agentic-ai/lib/python3.12/site-packages (from openai>=1.76.0->openai-agents==0.0.13) (0.9.0)\n",
      "Requirement already satisfied: tqdm>4 in /Users/tok/Dropbox/PARAL/Resources/virtual-envs/summit-agentic-ai/lib/python3.12/site-packages (from openai>=1.76.0->openai-agents==0.0.13) (4.67.1)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in /Users/tok/Dropbox/PARAL/Resources/virtual-envs/summit-agentic-ai/lib/python3.12/site-packages (from pydantic-settings>=2.5.2->mcp<2,>=1.6.0->openai-agents==0.0.13) (1.1.0)\n",
      "Requirement already satisfied: click>=7.0 in /Users/tok/Dropbox/PARAL/Resources/virtual-envs/summit-agentic-ai/lib/python3.12/site-packages (from uvicorn>=0.23.1->mcp<2,>=1.6.0->openai-agents==0.0.13) (8.1.8)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install \"openai-agents==0.0.13\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b24a40c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Certainly! Below is a simple example of a Python class called `User`. This class includes attributes for a user's name and email, as well as methods to get and set these attributes.\n",
      "\n",
      "```python\n",
      "class User:\n",
      "    def __init__(self, name, email):\n",
      "        self.name = name\n",
      "        self.email = email\n",
      "\n",
      "    def get_name(self):\n",
      "        return self.name\n",
      "\n",
      "    def set_name(self, name):\n",
      "        self.name = name\n",
      "\n",
      "    def get_email(self):\n",
      "        return self.email\n",
      "\n",
      "    def set_email(self, email):\n",
      "        self.email = email\n",
      "\n",
      "    def display_info(self):\n",
      "        print(f\"Name: {self.name}\")\n",
      "        print(f\"Email: {self.email}\")\n",
      "\n",
      "# Example usage:\n",
      "if __name__ == \"__main__\":\n",
      "    user1 = User(\"Alice\", \"alice@example.com\")\n",
      "    user1.display_info()\n",
      "\n",
      "    # Modifying user information\n",
      "    user1.set_name(\"Alice Smith\")\n",
      "    user1.set_email(\"alicesmith@example.com\")\n",
      "\n",
      "    print(\"\\nAfter updating:\")\n",
      "    user1.display_info()\n",
      "```\n",
      "\n",
      "### Explanation:\n",
      "1. ** `__init__` Method**: This is the constructor method that initializes a new instance of the `User` class with `name` and `email`.\n",
      "2. **Getter Methods (`get_name`, `get_email`)**: These methods return the current values of the `name` and `email` attributes.\n",
      "3. **Setter Methods (`set_name`, `set_email`)**: These methods allow you to update the `name` and `email` attributes.\n",
      "4. **`display_info` Method**: This method prints out the user's name and email.\n",
      "\n",
      "The example usage at the bottom shows how to create an instance of the `User` class, display its information, modify the information, and then display it again after the update.\n"
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "    \n",
    "    model=model,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful Python programming assistant .\"},\n",
    "        {\"role\": \"user\", \"content\": \"Write a simple Python example class called User\"}\n",
    "        \n",
    "    ]\n",
    ")\n",
    "\n",
    "print(f\"{response.choices[0].message.content}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "68489d9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error getting response: 404 page not found. (request_id: None)\n"
     ]
    },
    {
     "ename": "NotFoundError",
     "evalue": "404 page not found",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNotFoundError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[46]\u001b[39m\u001b[32m, line 23\u001b[39m\n\u001b[32m     14\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m data[\u001b[33m'\u001b[39m\u001b[33mcurrent\u001b[39m\u001b[33m'\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mtemperature_2m\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     16\u001b[39m agent = Agent(\n\u001b[32m     17\u001b[39m     name=\u001b[33m\"\u001b[39m\u001b[33mBlaster\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     18\u001b[39m     instructions=\u001b[33m\"\u001b[39m\u001b[33mAnswer the question asked very precisely. Please think before answering\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     19\u001b[39m     model=model,  \u001b[38;5;66;03m# Use the custom model instance\u001b[39;00m\n\u001b[32m     20\u001b[39m     tools=[get_weather],\n\u001b[32m     21\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m result = \u001b[38;5;28;01mawait\u001b[39;00m Runner.run(agent, \u001b[33m\"\u001b[39m\u001b[33mwhich is warmer now: Paris or Manila?\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     24\u001b[39m \u001b[38;5;28mprint\u001b[39m(result.final_output)\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# Use async syntax\u001b[39;00m\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# async def main():\u001b[39;00m\n\u001b[32m     28\u001b[39m \u001b[38;5;66;03m#     result = await Runner.run(agent, \"which is warmer now: Paris or Manila?\")\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     38\u001b[39m \u001b[38;5;66;03m# # asyncio.run(main())\u001b[39;00m\n\u001b[32m     39\u001b[39m \u001b[38;5;66;03m# await main()\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dropbox/PARAL/Resources/virtual-envs/summit-agentic-ai/lib/python3.12/site-packages/agents/run.py:218\u001b[39m, in \u001b[36mRunner.run\u001b[39m\u001b[34m(cls, starting_agent, input, context, max_turns, hooks, run_config, previous_response_id)\u001b[39m\n\u001b[32m    213\u001b[39m logger.debug(\n\u001b[32m    214\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mRunning agent \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcurrent_agent.name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m (turn \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcurrent_turn\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    215\u001b[39m )\n\u001b[32m    217\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m current_turn == \u001b[32m1\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m218\u001b[39m     input_guardrail_results, turn_result = \u001b[38;5;28;01mawait\u001b[39;00m asyncio.gather(\n\u001b[32m    219\u001b[39m         \u001b[38;5;28mcls\u001b[39m._run_input_guardrails(\n\u001b[32m    220\u001b[39m             starting_agent,\n\u001b[32m    221\u001b[39m             starting_agent.input_guardrails\n\u001b[32m    222\u001b[39m             + (run_config.input_guardrails \u001b[38;5;129;01mor\u001b[39;00m []),\n\u001b[32m    223\u001b[39m             copy.deepcopy(\u001b[38;5;28minput\u001b[39m),\n\u001b[32m    224\u001b[39m             context_wrapper,\n\u001b[32m    225\u001b[39m         ),\n\u001b[32m    226\u001b[39m         \u001b[38;5;28mcls\u001b[39m._run_single_turn(\n\u001b[32m    227\u001b[39m             agent=current_agent,\n\u001b[32m    228\u001b[39m             all_tools=all_tools,\n\u001b[32m    229\u001b[39m             original_input=original_input,\n\u001b[32m    230\u001b[39m             generated_items=generated_items,\n\u001b[32m    231\u001b[39m             hooks=hooks,\n\u001b[32m    232\u001b[39m             context_wrapper=context_wrapper,\n\u001b[32m    233\u001b[39m             run_config=run_config,\n\u001b[32m    234\u001b[39m             should_run_agent_start_hooks=should_run_agent_start_hooks,\n\u001b[32m    235\u001b[39m             tool_use_tracker=tool_use_tracker,\n\u001b[32m    236\u001b[39m             previous_response_id=previous_response_id,\n\u001b[32m    237\u001b[39m         ),\n\u001b[32m    238\u001b[39m     )\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    240\u001b[39m     turn_result = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mcls\u001b[39m._run_single_turn(\n\u001b[32m    241\u001b[39m         agent=current_agent,\n\u001b[32m    242\u001b[39m         all_tools=all_tools,\n\u001b[32m   (...)\u001b[39m\u001b[32m    250\u001b[39m         previous_response_id=previous_response_id,\n\u001b[32m    251\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dropbox/PARAL/Resources/virtual-envs/summit-agentic-ai/lib/python3.12/site-packages/agents/run.py:757\u001b[39m, in \u001b[36mRunner._run_single_turn\u001b[39m\u001b[34m(cls, agent, all_tools, original_input, generated_items, hooks, context_wrapper, run_config, should_run_agent_start_hooks, tool_use_tracker, previous_response_id)\u001b[39m\n\u001b[32m    754\u001b[39m \u001b[38;5;28minput\u001b[39m = ItemHelpers.input_to_new_input_list(original_input)\n\u001b[32m    755\u001b[39m \u001b[38;5;28minput\u001b[39m.extend([generated_item.to_input_item() \u001b[38;5;28;01mfor\u001b[39;00m generated_item \u001b[38;5;129;01min\u001b[39;00m generated_items])\n\u001b[32m--> \u001b[39m\u001b[32m757\u001b[39m new_response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mcls\u001b[39m._get_new_response(\n\u001b[32m    758\u001b[39m     agent,\n\u001b[32m    759\u001b[39m     system_prompt,\n\u001b[32m    760\u001b[39m     \u001b[38;5;28minput\u001b[39m,\n\u001b[32m    761\u001b[39m     output_schema,\n\u001b[32m    762\u001b[39m     all_tools,\n\u001b[32m    763\u001b[39m     handoffs,\n\u001b[32m    764\u001b[39m     context_wrapper,\n\u001b[32m    765\u001b[39m     run_config,\n\u001b[32m    766\u001b[39m     tool_use_tracker,\n\u001b[32m    767\u001b[39m     previous_response_id,\n\u001b[32m    768\u001b[39m )\n\u001b[32m    770\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mcls\u001b[39m._get_single_step_result_from_response(\n\u001b[32m    771\u001b[39m     agent=agent,\n\u001b[32m    772\u001b[39m     original_input=original_input,\n\u001b[32m   (...)\u001b[39m\u001b[32m    781\u001b[39m     tool_use_tracker=tool_use_tracker,\n\u001b[32m    782\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dropbox/PARAL/Resources/virtual-envs/summit-agentic-ai/lib/python3.12/site-packages/agents/run.py:916\u001b[39m, in \u001b[36mRunner._get_new_response\u001b[39m\u001b[34m(cls, agent, system_prompt, input, output_schema, all_tools, handoffs, context_wrapper, run_config, tool_use_tracker, previous_response_id)\u001b[39m\n\u001b[32m    913\u001b[39m model_settings = agent.model_settings.resolve(run_config.model_settings)\n\u001b[32m    914\u001b[39m model_settings = RunImpl.maybe_reset_tool_choice(agent, tool_use_tracker, model_settings)\n\u001b[32m--> \u001b[39m\u001b[32m916\u001b[39m new_response = \u001b[38;5;28;01mawait\u001b[39;00m model.get_response(\n\u001b[32m    917\u001b[39m     system_instructions=system_prompt,\n\u001b[32m    918\u001b[39m     \u001b[38;5;28minput\u001b[39m=\u001b[38;5;28minput\u001b[39m,\n\u001b[32m    919\u001b[39m     model_settings=model_settings,\n\u001b[32m    920\u001b[39m     tools=all_tools,\n\u001b[32m    921\u001b[39m     output_schema=output_schema,\n\u001b[32m    922\u001b[39m     handoffs=handoffs,\n\u001b[32m    923\u001b[39m     tracing=get_model_tracing_impl(\n\u001b[32m    924\u001b[39m         run_config.tracing_disabled, run_config.trace_include_sensitive_data\n\u001b[32m    925\u001b[39m     ),\n\u001b[32m    926\u001b[39m     previous_response_id=previous_response_id,\n\u001b[32m    927\u001b[39m )\n\u001b[32m    929\u001b[39m context_wrapper.usage.add(new_response.usage)\n\u001b[32m    931\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m new_response\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dropbox/PARAL/Resources/virtual-envs/summit-agentic-ai/lib/python3.12/site-packages/agents/models/openai_responses.py:76\u001b[39m, in \u001b[36mOpenAIResponsesModel.get_response\u001b[39m\u001b[34m(self, system_instructions, input, model_settings, tools, output_schema, handoffs, tracing, previous_response_id)\u001b[39m\n\u001b[32m     74\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m response_span(disabled=tracing.is_disabled()) \u001b[38;5;28;01mas\u001b[39;00m span_response:\n\u001b[32m     75\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m76\u001b[39m         response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._fetch_response(\n\u001b[32m     77\u001b[39m             system_instructions,\n\u001b[32m     78\u001b[39m             \u001b[38;5;28minput\u001b[39m,\n\u001b[32m     79\u001b[39m             model_settings,\n\u001b[32m     80\u001b[39m             tools,\n\u001b[32m     81\u001b[39m             output_schema,\n\u001b[32m     82\u001b[39m             handoffs,\n\u001b[32m     83\u001b[39m             previous_response_id,\n\u001b[32m     84\u001b[39m             stream=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m     85\u001b[39m         )\n\u001b[32m     87\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m _debug.DONT_LOG_MODEL_DATA:\n\u001b[32m     88\u001b[39m             logger.debug(\u001b[33m\"\u001b[39m\u001b[33mLLM responded\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dropbox/PARAL/Resources/virtual-envs/summit-agentic-ai/lib/python3.12/site-packages/agents/models/openai_responses.py:242\u001b[39m, in \u001b[36mOpenAIResponsesModel._fetch_response\u001b[39m\u001b[34m(self, system_instructions, input, model_settings, tools, output_schema, handoffs, previous_response_id, stream)\u001b[39m\n\u001b[32m    231\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    232\u001b[39m     logger.debug(\n\u001b[32m    233\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCalling LLM \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.model\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m with input:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    234\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mjson.dumps(list_input,\u001b[38;5;250m \u001b[39mindent=\u001b[32m2\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    239\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPrevious response id: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprevious_response_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    240\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m242\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._client.responses.create(\n\u001b[32m    243\u001b[39m     previous_response_id=\u001b[38;5;28mself\u001b[39m._non_null_or_not_given(previous_response_id),\n\u001b[32m    244\u001b[39m     instructions=\u001b[38;5;28mself\u001b[39m._non_null_or_not_given(system_instructions),\n\u001b[32m    245\u001b[39m     model=\u001b[38;5;28mself\u001b[39m.model,\n\u001b[32m    246\u001b[39m     \u001b[38;5;28minput\u001b[39m=list_input,\n\u001b[32m    247\u001b[39m     include=converted_tools.includes,\n\u001b[32m    248\u001b[39m     tools=converted_tools.tools,\n\u001b[32m    249\u001b[39m     temperature=\u001b[38;5;28mself\u001b[39m._non_null_or_not_given(model_settings.temperature),\n\u001b[32m    250\u001b[39m     top_p=\u001b[38;5;28mself\u001b[39m._non_null_or_not_given(model_settings.top_p),\n\u001b[32m    251\u001b[39m     truncation=\u001b[38;5;28mself\u001b[39m._non_null_or_not_given(model_settings.truncation),\n\u001b[32m    252\u001b[39m     max_output_tokens=\u001b[38;5;28mself\u001b[39m._non_null_or_not_given(model_settings.max_tokens),\n\u001b[32m    253\u001b[39m     tool_choice=tool_choice,\n\u001b[32m    254\u001b[39m     parallel_tool_calls=parallel_tool_calls,\n\u001b[32m    255\u001b[39m     stream=stream,\n\u001b[32m    256\u001b[39m     extra_headers={**_HEADERS, **(model_settings.extra_headers \u001b[38;5;129;01mor\u001b[39;00m {})},\n\u001b[32m    257\u001b[39m     extra_query=model_settings.extra_query,\n\u001b[32m    258\u001b[39m     extra_body=model_settings.extra_body,\n\u001b[32m    259\u001b[39m     text=response_format,\n\u001b[32m    260\u001b[39m     store=\u001b[38;5;28mself\u001b[39m._non_null_or_not_given(model_settings.store),\n\u001b[32m    261\u001b[39m     reasoning=\u001b[38;5;28mself\u001b[39m._non_null_or_not_given(model_settings.reasoning),\n\u001b[32m    262\u001b[39m     metadata=\u001b[38;5;28mself\u001b[39m._non_null_or_not_given(model_settings.metadata),\n\u001b[32m    263\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dropbox/PARAL/Resources/virtual-envs/summit-agentic-ai/lib/python3.12/site-packages/openai/resources/responses/responses.py:1529\u001b[39m, in \u001b[36mAsyncResponses.create\u001b[39m\u001b[34m(self, input, model, include, instructions, max_output_tokens, metadata, parallel_tool_calls, previous_response_id, reasoning, service_tier, store, stream, temperature, text, tool_choice, tools, top_p, truncation, user, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m   1499\u001b[39m \u001b[38;5;129m@required_args\u001b[39m([\u001b[33m\"\u001b[39m\u001b[33minput\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m], [\u001b[33m\"\u001b[39m\u001b[33minput\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m   1500\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m   1501\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1527\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = NOT_GIVEN,\n\u001b[32m   1528\u001b[39m ) -> Response | AsyncStream[ResponseStreamEvent]:\n\u001b[32m-> \u001b[39m\u001b[32m1529\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._post(\n\u001b[32m   1530\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m/responses\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1531\u001b[39m         body=\u001b[38;5;28;01mawait\u001b[39;00m async_maybe_transform(\n\u001b[32m   1532\u001b[39m             {\n\u001b[32m   1533\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33minput\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28minput\u001b[39m,\n\u001b[32m   1534\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m: model,\n\u001b[32m   1535\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33minclude\u001b[39m\u001b[33m\"\u001b[39m: include,\n\u001b[32m   1536\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33minstructions\u001b[39m\u001b[33m\"\u001b[39m: instructions,\n\u001b[32m   1537\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmax_output_tokens\u001b[39m\u001b[33m\"\u001b[39m: max_output_tokens,\n\u001b[32m   1538\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: metadata,\n\u001b[32m   1539\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mparallel_tool_calls\u001b[39m\u001b[33m\"\u001b[39m: parallel_tool_calls,\n\u001b[32m   1540\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mprevious_response_id\u001b[39m\u001b[33m\"\u001b[39m: previous_response_id,\n\u001b[32m   1541\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mreasoning\u001b[39m\u001b[33m\"\u001b[39m: reasoning,\n\u001b[32m   1542\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mservice_tier\u001b[39m\u001b[33m\"\u001b[39m: service_tier,\n\u001b[32m   1543\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstore\u001b[39m\u001b[33m\"\u001b[39m: store,\n\u001b[32m   1544\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m: stream,\n\u001b[32m   1545\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtemperature\u001b[39m\u001b[33m\"\u001b[39m: temperature,\n\u001b[32m   1546\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m: text,\n\u001b[32m   1547\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtool_choice\u001b[39m\u001b[33m\"\u001b[39m: tool_choice,\n\u001b[32m   1548\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtools\u001b[39m\u001b[33m\"\u001b[39m: tools,\n\u001b[32m   1549\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtop_p\u001b[39m\u001b[33m\"\u001b[39m: top_p,\n\u001b[32m   1550\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtruncation\u001b[39m\u001b[33m\"\u001b[39m: truncation,\n\u001b[32m   1551\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m: user,\n\u001b[32m   1552\u001b[39m             },\n\u001b[32m   1553\u001b[39m             response_create_params.ResponseCreateParamsStreaming\n\u001b[32m   1554\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m stream\n\u001b[32m   1555\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m response_create_params.ResponseCreateParamsNonStreaming,\n\u001b[32m   1556\u001b[39m         ),\n\u001b[32m   1557\u001b[39m         options=make_request_options(\n\u001b[32m   1558\u001b[39m             extra_headers=extra_headers, extra_query=extra_query, extra_body=extra_body, timeout=timeout\n\u001b[32m   1559\u001b[39m         ),\n\u001b[32m   1560\u001b[39m         cast_to=Response,\n\u001b[32m   1561\u001b[39m         stream=stream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m   1562\u001b[39m         stream_cls=AsyncStream[ResponseStreamEvent],\n\u001b[32m   1563\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dropbox/PARAL/Resources/virtual-envs/summit-agentic-ai/lib/python3.12/site-packages/openai/_base_client.py:1742\u001b[39m, in \u001b[36mAsyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, files, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1728\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1729\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1730\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1737\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_AsyncStreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1738\u001b[39m ) -> ResponseT | _AsyncStreamT:\n\u001b[32m   1739\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1740\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=\u001b[38;5;28;01mawait\u001b[39;00m async_to_httpx_files(files), **options\n\u001b[32m   1741\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1742\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dropbox/PARAL/Resources/virtual-envs/summit-agentic-ai/lib/python3.12/site-packages/openai/_base_client.py:1549\u001b[39m, in \u001b[36mAsyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1546\u001b[39m             \u001b[38;5;28;01mawait\u001b[39;00m err.response.aread()\n\u001b[32m   1548\u001b[39m         log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1549\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1551\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1553\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mcould not resolve response (should never happen)\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mNotFoundError\u001b[39m: 404 page not found"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from agents import Agent, ModelSettings, function_tool, Runner, AsyncOpenAI, OpenAIChatCompletionsModel, set_default_openai_client\n",
    "\n",
    "# Create custom client pointing to Ollama\n",
    "external_client = AsyncOpenAI(\n",
    "    base_url='http://localhost:11434/v1',\n",
    "    api_key='ollama'  # Required but unused\n",
    ")\n",
    "\n",
    "@function_tool\n",
    "def get_weather(latitude: str, longitude: str) -> str:\n",
    "    response = requests.get(f\"https://api.open-meteo.com/v1/forecast?latitude={latitude}&longitude={longitude}&current=temperature_2m,wind_speed_10m&hourly=temperature_2m,relative_humidity_2m,wind_speed_10m\")\n",
    "    data = response.json()\n",
    "    return data['current']['temperature_2m']\n",
    "\n",
    "agent = Agent(\n",
    "    name=\"Blaster\",\n",
    "    instructions=\"Answer the question asked very precisely. Please think before answering\",\n",
    "    model=model,  # Use the custom model instance\n",
    "    tools=[get_weather],\n",
    ")\n",
    "\n",
    "result = await Runner.run(agent, \"which is warmer now: Paris or Manila?\")\n",
    "print(result.final_output)\n",
    "\n",
    "# Use async syntax\n",
    "# async def main():\n",
    "#     result = await Runner.run(agent, \"which is warmer now: Paris or Manila?\")\n",
    "#     print(result.final_output)\n",
    "    \n",
    "#     # Uncomment the line below to see the detailed interactions including automatic tool calling\n",
    "#     print(result)\n",
    "\n",
    "# # Run the async function\n",
    "# import asyncio\n",
    "# # asyncio.run(main())\n",
    "# # If you are using Jupyter Notebook, use the following line instead:\n",
    "# # asyncio.run(main())\n",
    "# await main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbcfc1af",
   "metadata": {},
   "source": [
    "import requests\n",
    "from agents import Agent, ModelSettings, function_tool, Runner, AsyncOpenAI, OpenAIChatCompletionsModel\n",
    "\n",
    "@function_tool\n",
    "def get_weather(latitude: str, longitude: str) -> str:\n",
    "    response = requests.get(f\"https://api.open-meteo.com/v1/forecast?latitude={latitude}&longitude={longitude}&current=temperature_2m,wind_speed_10m&hourly=temperature_2m,relative_humidity_2m,wind_speed_10m\")\n",
    "    data = response.json()\n",
    "    return data['current']['temperature_2m']\n",
    "\n",
    "agent = Agent(\n",
    "    name=\"Blaster\",\n",
    "    instructions=\"Answer the question asked very precisely. Please think before answering\",\n",
    "    model=model,  # Use the custom model settings instead of just the model name\n",
    "    tools=[get_weather],\n",
    ")\n",
    "\n",
    "# Use async syntax\n",
    "async def main():\n",
    "    result = await Runner.run(agent, \"which is warmer now: Paris or Manila?\")\n",
    "    print(result.final_output)\n",
    "    \n",
    "    # Uncomment the line below to see the detailed interactions including automatic tool calling\n",
    "    print(result)\n",
    "\n",
    "# Run the async function\n",
    "import asyncio\n",
    "asyncio.run(main())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74024ca-e0c2-43f2-ab80-7f068df322ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error getting response: 404 page not found. (request_id: None)\n"
     ]
    },
    {
     "ename": "NotFoundError",
     "evalue": "404 page not found",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNotFoundError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[40]\u001b[39m\u001b[32m, line 24\u001b[39m\n\u001b[32m     15\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m data[\u001b[33m'\u001b[39m\u001b[33mcurrent\u001b[39m\u001b[33m'\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mtemperature_2m\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     17\u001b[39m agent = Agent(\n\u001b[32m     18\u001b[39m     name=\u001b[33m\"\u001b[39m\u001b[33mBlaster\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     19\u001b[39m     instructions=\u001b[33m\"\u001b[39m\u001b[33mAnswer the question asked very precisely. Please think before answering\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     20\u001b[39m     model=\u001b[33m\"\u001b[39m\u001b[33mqwen3:32b\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     21\u001b[39m     tools=[get_weather],\n\u001b[32m     22\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m result = \u001b[38;5;28;01mawait\u001b[39;00m Runner.run(agent, \u001b[33m\"\u001b[39m\u001b[33mwhich is warmer now: Paris or Manila?\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     25\u001b[39m \u001b[38;5;28mprint\u001b[39m(result.final_output)\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m#uncomment the line below to see the detailed interactions including automatic tool calling\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dropbox/PARAL/Resources/virtual-envs/summit-agentic-ai/lib/python3.12/site-packages/agents/run.py:218\u001b[39m, in \u001b[36mRunner.run\u001b[39m\u001b[34m(cls, starting_agent, input, context, max_turns, hooks, run_config, previous_response_id)\u001b[39m\n\u001b[32m    213\u001b[39m logger.debug(\n\u001b[32m    214\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mRunning agent \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcurrent_agent.name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m (turn \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcurrent_turn\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    215\u001b[39m )\n\u001b[32m    217\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m current_turn == \u001b[32m1\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m218\u001b[39m     input_guardrail_results, turn_result = \u001b[38;5;28;01mawait\u001b[39;00m asyncio.gather(\n\u001b[32m    219\u001b[39m         \u001b[38;5;28mcls\u001b[39m._run_input_guardrails(\n\u001b[32m    220\u001b[39m             starting_agent,\n\u001b[32m    221\u001b[39m             starting_agent.input_guardrails\n\u001b[32m    222\u001b[39m             + (run_config.input_guardrails \u001b[38;5;129;01mor\u001b[39;00m []),\n\u001b[32m    223\u001b[39m             copy.deepcopy(\u001b[38;5;28minput\u001b[39m),\n\u001b[32m    224\u001b[39m             context_wrapper,\n\u001b[32m    225\u001b[39m         ),\n\u001b[32m    226\u001b[39m         \u001b[38;5;28mcls\u001b[39m._run_single_turn(\n\u001b[32m    227\u001b[39m             agent=current_agent,\n\u001b[32m    228\u001b[39m             all_tools=all_tools,\n\u001b[32m    229\u001b[39m             original_input=original_input,\n\u001b[32m    230\u001b[39m             generated_items=generated_items,\n\u001b[32m    231\u001b[39m             hooks=hooks,\n\u001b[32m    232\u001b[39m             context_wrapper=context_wrapper,\n\u001b[32m    233\u001b[39m             run_config=run_config,\n\u001b[32m    234\u001b[39m             should_run_agent_start_hooks=should_run_agent_start_hooks,\n\u001b[32m    235\u001b[39m             tool_use_tracker=tool_use_tracker,\n\u001b[32m    236\u001b[39m             previous_response_id=previous_response_id,\n\u001b[32m    237\u001b[39m         ),\n\u001b[32m    238\u001b[39m     )\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    240\u001b[39m     turn_result = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mcls\u001b[39m._run_single_turn(\n\u001b[32m    241\u001b[39m         agent=current_agent,\n\u001b[32m    242\u001b[39m         all_tools=all_tools,\n\u001b[32m   (...)\u001b[39m\u001b[32m    250\u001b[39m         previous_response_id=previous_response_id,\n\u001b[32m    251\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dropbox/PARAL/Resources/virtual-envs/summit-agentic-ai/lib/python3.12/site-packages/agents/run.py:757\u001b[39m, in \u001b[36mRunner._run_single_turn\u001b[39m\u001b[34m(cls, agent, all_tools, original_input, generated_items, hooks, context_wrapper, run_config, should_run_agent_start_hooks, tool_use_tracker, previous_response_id)\u001b[39m\n\u001b[32m    754\u001b[39m \u001b[38;5;28minput\u001b[39m = ItemHelpers.input_to_new_input_list(original_input)\n\u001b[32m    755\u001b[39m \u001b[38;5;28minput\u001b[39m.extend([generated_item.to_input_item() \u001b[38;5;28;01mfor\u001b[39;00m generated_item \u001b[38;5;129;01min\u001b[39;00m generated_items])\n\u001b[32m--> \u001b[39m\u001b[32m757\u001b[39m new_response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mcls\u001b[39m._get_new_response(\n\u001b[32m    758\u001b[39m     agent,\n\u001b[32m    759\u001b[39m     system_prompt,\n\u001b[32m    760\u001b[39m     \u001b[38;5;28minput\u001b[39m,\n\u001b[32m    761\u001b[39m     output_schema,\n\u001b[32m    762\u001b[39m     all_tools,\n\u001b[32m    763\u001b[39m     handoffs,\n\u001b[32m    764\u001b[39m     context_wrapper,\n\u001b[32m    765\u001b[39m     run_config,\n\u001b[32m    766\u001b[39m     tool_use_tracker,\n\u001b[32m    767\u001b[39m     previous_response_id,\n\u001b[32m    768\u001b[39m )\n\u001b[32m    770\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mcls\u001b[39m._get_single_step_result_from_response(\n\u001b[32m    771\u001b[39m     agent=agent,\n\u001b[32m    772\u001b[39m     original_input=original_input,\n\u001b[32m   (...)\u001b[39m\u001b[32m    781\u001b[39m     tool_use_tracker=tool_use_tracker,\n\u001b[32m    782\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dropbox/PARAL/Resources/virtual-envs/summit-agentic-ai/lib/python3.12/site-packages/agents/run.py:916\u001b[39m, in \u001b[36mRunner._get_new_response\u001b[39m\u001b[34m(cls, agent, system_prompt, input, output_schema, all_tools, handoffs, context_wrapper, run_config, tool_use_tracker, previous_response_id)\u001b[39m\n\u001b[32m    913\u001b[39m model_settings = agent.model_settings.resolve(run_config.model_settings)\n\u001b[32m    914\u001b[39m model_settings = RunImpl.maybe_reset_tool_choice(agent, tool_use_tracker, model_settings)\n\u001b[32m--> \u001b[39m\u001b[32m916\u001b[39m new_response = \u001b[38;5;28;01mawait\u001b[39;00m model.get_response(\n\u001b[32m    917\u001b[39m     system_instructions=system_prompt,\n\u001b[32m    918\u001b[39m     \u001b[38;5;28minput\u001b[39m=\u001b[38;5;28minput\u001b[39m,\n\u001b[32m    919\u001b[39m     model_settings=model_settings,\n\u001b[32m    920\u001b[39m     tools=all_tools,\n\u001b[32m    921\u001b[39m     output_schema=output_schema,\n\u001b[32m    922\u001b[39m     handoffs=handoffs,\n\u001b[32m    923\u001b[39m     tracing=get_model_tracing_impl(\n\u001b[32m    924\u001b[39m         run_config.tracing_disabled, run_config.trace_include_sensitive_data\n\u001b[32m    925\u001b[39m     ),\n\u001b[32m    926\u001b[39m     previous_response_id=previous_response_id,\n\u001b[32m    927\u001b[39m )\n\u001b[32m    929\u001b[39m context_wrapper.usage.add(new_response.usage)\n\u001b[32m    931\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m new_response\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dropbox/PARAL/Resources/virtual-envs/summit-agentic-ai/lib/python3.12/site-packages/agents/models/openai_responses.py:76\u001b[39m, in \u001b[36mOpenAIResponsesModel.get_response\u001b[39m\u001b[34m(self, system_instructions, input, model_settings, tools, output_schema, handoffs, tracing, previous_response_id)\u001b[39m\n\u001b[32m     74\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m response_span(disabled=tracing.is_disabled()) \u001b[38;5;28;01mas\u001b[39;00m span_response:\n\u001b[32m     75\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m76\u001b[39m         response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._fetch_response(\n\u001b[32m     77\u001b[39m             system_instructions,\n\u001b[32m     78\u001b[39m             \u001b[38;5;28minput\u001b[39m,\n\u001b[32m     79\u001b[39m             model_settings,\n\u001b[32m     80\u001b[39m             tools,\n\u001b[32m     81\u001b[39m             output_schema,\n\u001b[32m     82\u001b[39m             handoffs,\n\u001b[32m     83\u001b[39m             previous_response_id,\n\u001b[32m     84\u001b[39m             stream=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m     85\u001b[39m         )\n\u001b[32m     87\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m _debug.DONT_LOG_MODEL_DATA:\n\u001b[32m     88\u001b[39m             logger.debug(\u001b[33m\"\u001b[39m\u001b[33mLLM responded\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dropbox/PARAL/Resources/virtual-envs/summit-agentic-ai/lib/python3.12/site-packages/agents/models/openai_responses.py:242\u001b[39m, in \u001b[36mOpenAIResponsesModel._fetch_response\u001b[39m\u001b[34m(self, system_instructions, input, model_settings, tools, output_schema, handoffs, previous_response_id, stream)\u001b[39m\n\u001b[32m    231\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    232\u001b[39m     logger.debug(\n\u001b[32m    233\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCalling LLM \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.model\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m with input:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    234\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mjson.dumps(list_input,\u001b[38;5;250m \u001b[39mindent=\u001b[32m2\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    239\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPrevious response id: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprevious_response_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    240\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m242\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._client.responses.create(\n\u001b[32m    243\u001b[39m     previous_response_id=\u001b[38;5;28mself\u001b[39m._non_null_or_not_given(previous_response_id),\n\u001b[32m    244\u001b[39m     instructions=\u001b[38;5;28mself\u001b[39m._non_null_or_not_given(system_instructions),\n\u001b[32m    245\u001b[39m     model=\u001b[38;5;28mself\u001b[39m.model,\n\u001b[32m    246\u001b[39m     \u001b[38;5;28minput\u001b[39m=list_input,\n\u001b[32m    247\u001b[39m     include=converted_tools.includes,\n\u001b[32m    248\u001b[39m     tools=converted_tools.tools,\n\u001b[32m    249\u001b[39m     temperature=\u001b[38;5;28mself\u001b[39m._non_null_or_not_given(model_settings.temperature),\n\u001b[32m    250\u001b[39m     top_p=\u001b[38;5;28mself\u001b[39m._non_null_or_not_given(model_settings.top_p),\n\u001b[32m    251\u001b[39m     truncation=\u001b[38;5;28mself\u001b[39m._non_null_or_not_given(model_settings.truncation),\n\u001b[32m    252\u001b[39m     max_output_tokens=\u001b[38;5;28mself\u001b[39m._non_null_or_not_given(model_settings.max_tokens),\n\u001b[32m    253\u001b[39m     tool_choice=tool_choice,\n\u001b[32m    254\u001b[39m     parallel_tool_calls=parallel_tool_calls,\n\u001b[32m    255\u001b[39m     stream=stream,\n\u001b[32m    256\u001b[39m     extra_headers={**_HEADERS, **(model_settings.extra_headers \u001b[38;5;129;01mor\u001b[39;00m {})},\n\u001b[32m    257\u001b[39m     extra_query=model_settings.extra_query,\n\u001b[32m    258\u001b[39m     extra_body=model_settings.extra_body,\n\u001b[32m    259\u001b[39m     text=response_format,\n\u001b[32m    260\u001b[39m     store=\u001b[38;5;28mself\u001b[39m._non_null_or_not_given(model_settings.store),\n\u001b[32m    261\u001b[39m     reasoning=\u001b[38;5;28mself\u001b[39m._non_null_or_not_given(model_settings.reasoning),\n\u001b[32m    262\u001b[39m     metadata=\u001b[38;5;28mself\u001b[39m._non_null_or_not_given(model_settings.metadata),\n\u001b[32m    263\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dropbox/PARAL/Resources/virtual-envs/summit-agentic-ai/lib/python3.12/site-packages/openai/resources/responses/responses.py:1529\u001b[39m, in \u001b[36mAsyncResponses.create\u001b[39m\u001b[34m(self, input, model, include, instructions, max_output_tokens, metadata, parallel_tool_calls, previous_response_id, reasoning, service_tier, store, stream, temperature, text, tool_choice, tools, top_p, truncation, user, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m   1499\u001b[39m \u001b[38;5;129m@required_args\u001b[39m([\u001b[33m\"\u001b[39m\u001b[33minput\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m], [\u001b[33m\"\u001b[39m\u001b[33minput\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m   1500\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m   1501\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1527\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = NOT_GIVEN,\n\u001b[32m   1528\u001b[39m ) -> Response | AsyncStream[ResponseStreamEvent]:\n\u001b[32m-> \u001b[39m\u001b[32m1529\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._post(\n\u001b[32m   1530\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m/responses\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1531\u001b[39m         body=\u001b[38;5;28;01mawait\u001b[39;00m async_maybe_transform(\n\u001b[32m   1532\u001b[39m             {\n\u001b[32m   1533\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33minput\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28minput\u001b[39m,\n\u001b[32m   1534\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m: model,\n\u001b[32m   1535\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33minclude\u001b[39m\u001b[33m\"\u001b[39m: include,\n\u001b[32m   1536\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33minstructions\u001b[39m\u001b[33m\"\u001b[39m: instructions,\n\u001b[32m   1537\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmax_output_tokens\u001b[39m\u001b[33m\"\u001b[39m: max_output_tokens,\n\u001b[32m   1538\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: metadata,\n\u001b[32m   1539\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mparallel_tool_calls\u001b[39m\u001b[33m\"\u001b[39m: parallel_tool_calls,\n\u001b[32m   1540\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mprevious_response_id\u001b[39m\u001b[33m\"\u001b[39m: previous_response_id,\n\u001b[32m   1541\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mreasoning\u001b[39m\u001b[33m\"\u001b[39m: reasoning,\n\u001b[32m   1542\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mservice_tier\u001b[39m\u001b[33m\"\u001b[39m: service_tier,\n\u001b[32m   1543\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstore\u001b[39m\u001b[33m\"\u001b[39m: store,\n\u001b[32m   1544\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m: stream,\n\u001b[32m   1545\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtemperature\u001b[39m\u001b[33m\"\u001b[39m: temperature,\n\u001b[32m   1546\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m: text,\n\u001b[32m   1547\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtool_choice\u001b[39m\u001b[33m\"\u001b[39m: tool_choice,\n\u001b[32m   1548\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtools\u001b[39m\u001b[33m\"\u001b[39m: tools,\n\u001b[32m   1549\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtop_p\u001b[39m\u001b[33m\"\u001b[39m: top_p,\n\u001b[32m   1550\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtruncation\u001b[39m\u001b[33m\"\u001b[39m: truncation,\n\u001b[32m   1551\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m: user,\n\u001b[32m   1552\u001b[39m             },\n\u001b[32m   1553\u001b[39m             response_create_params.ResponseCreateParamsStreaming\n\u001b[32m   1554\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m stream\n\u001b[32m   1555\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m response_create_params.ResponseCreateParamsNonStreaming,\n\u001b[32m   1556\u001b[39m         ),\n\u001b[32m   1557\u001b[39m         options=make_request_options(\n\u001b[32m   1558\u001b[39m             extra_headers=extra_headers, extra_query=extra_query, extra_body=extra_body, timeout=timeout\n\u001b[32m   1559\u001b[39m         ),\n\u001b[32m   1560\u001b[39m         cast_to=Response,\n\u001b[32m   1561\u001b[39m         stream=stream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m   1562\u001b[39m         stream_cls=AsyncStream[ResponseStreamEvent],\n\u001b[32m   1563\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dropbox/PARAL/Resources/virtual-envs/summit-agentic-ai/lib/python3.12/site-packages/openai/_base_client.py:1742\u001b[39m, in \u001b[36mAsyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, files, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1728\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1729\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1730\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1737\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_AsyncStreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1738\u001b[39m ) -> ResponseT | _AsyncStreamT:\n\u001b[32m   1739\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1740\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=\u001b[38;5;28;01mawait\u001b[39;00m async_to_httpx_files(files), **options\n\u001b[32m   1741\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1742\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dropbox/PARAL/Resources/virtual-envs/summit-agentic-ai/lib/python3.12/site-packages/openai/_base_client.py:1549\u001b[39m, in \u001b[36mAsyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1546\u001b[39m             \u001b[38;5;28;01mawait\u001b[39;00m err.response.aread()\n\u001b[32m   1548\u001b[39m         log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1549\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1551\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1553\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mcould not resolve response (should never happen)\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mNotFoundError\u001b[39m: 404 page not found"
     ]
    }
   ],
   "source": [
    "from agents import Agent, ModelSettings, function_tool, Runner, AsyncOpenAI, OpenAIChatCompletionsModel, set_default_openai_client\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "@function_tool\n",
    "def get_weather(latitude:str, longitude:str) ->str:\n",
    "    response = requests.get(f\"https://api.open-meteo.com/v1/forecast?latitude={latitude}&longitude={longitude}&current=temperature_2m,wind_speed_10m&hourly=temperature_2m,relative_humidity_2m,wind_speed_10m\")\n",
    "    data = response.json()\n",
    "    return data['current']['temperature_2m']\n",
    "   \n",
    "agent = Agent(\n",
    "    name=\"Blaster\",\n",
    "    instructions=\"Answer the question asked very precisely. Please think before answering\",\n",
    "    model=\"qwen3:32b\",\n",
    "    tools=[get_weather],\n",
    ")\n",
    "\n",
    "result = await Runner.run(agent, \"which is warmer now: Paris or Manila?\")\n",
    "print(result.final_output)\n",
    "\n",
    "#uncomment the line below to see the detailed interactions including automatic tool calling\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0077ab16-b9e5-4699-a46e-6b64278cbda4",
   "metadata": {},
   "source": [
    "# Tools\n",
    "- We are definining tools with an adornment here.\n",
    "- MCP Servers are formalizing this much more and everyone is adopting this.\n",
    "- This is covered in another lab."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87ee407-2c3e-4f48-8f28-9110ba975f01",
   "metadata": {},
   "source": [
    "# Agentic Patterns\n",
    "\n",
    "\n",
    "We explore below 3 agentic paradigms which are widely used\n",
    "\n",
    "1. Agents collaborating with each other to improve the quality of the output\n",
    "   ![Collaboration Pattern](resources/images/agent_collaborate.png)\n",
    "3. Agents routing traffic to the correct agent\n",
    "   ![pattern-1](resources/images/agent_supervisor_pattern.png)  \n",
    "   ![pattern-2](resources/images/agent_hierarchical.png) \n",
    "5. Agents running a workflow\n",
    "   ![Workflow Pattern](resources/images/agent_plan_execute.png)\n",
    "\n",
    "There are other agentic patterns as well which we do not cover here. But these basic concepts should help adopting other patterns much simpler.\n",
    "\n",
    "_The graphics have been used from [langraph tutorial](https://github.com/langchain-ai/langgraph/blob/main/docs/docs/tutorials)_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c11e8b-6396-499b-abec-a8b674bdf312",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Agents Collaborating\n",
    "1. Simply demonstrates an agent reviewing the work of another agent - much like a human being.\n",
    "1. This is one of the primary reasons while the agents can help increase accuracty of the answer and smaller models using agents can outperform larger models without agents.\n",
    "1. This pattern can be used in lots of scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e676d7f-a94f-4add-84e2-e7d8074bfba7",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'contrib'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdataclasses\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m dataclass\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Literal\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01magents\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Agent, ItemHelpers, Runner, TResponseInputItem, trace\n\u001b[32m      6\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[33;03mThis example shows the agents collaborating or one agent reveiwing the work of another and giving feedback. \u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[33;03mThe first agent generates an outline for a story.\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[33;03mThe second agent judges the outline and provides feedback. \u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[33;03mWe loop until the judge is satisfied with the outline.\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# model = \"gpt-4o\"\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dropbox/PARAL/Resources/virtual-envs/summit-agentic-ai/lib/python3.12/site-packages/agents/__init__.py:22\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m__future__\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m print_function\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m algorithms\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m scripts\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tools\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dropbox/PARAL/Resources/virtual-envs/summit-agentic-ai/lib/python3.12/site-packages/agents/scripts/__init__.py:21\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m__future__\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m division\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m__future__\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m print_function\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m train\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m utility\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m visualize\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dropbox/PARAL/Resources/virtual-envs/summit-agentic-ai/lib/python3.12/site-packages/agents/scripts/train.py:33\u001b[39m\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtf\u001b[39;00m\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01magents\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tools\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01magents\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mscripts\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m configs\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01magents\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mscripts\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m utility\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_create_environment\u001b[39m(config):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dropbox/PARAL/Resources/virtual-envs/summit-agentic-ai/lib/python3.12/site-packages/agents/scripts/configs.py:26\u001b[39m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtf\u001b[39;00m\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01magents\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m algorithms\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01magents\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mscripts\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m networks\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdefault\u001b[39m():\n\u001b[32m     30\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Default configuration for PPO.\"\"\"\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dropbox/PARAL/Resources/virtual-envs/summit-agentic-ai/lib/python3.12/site-packages/agents/scripts/networks.py:30\u001b[39m\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtf\u001b[39;00m\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01magents\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m tfd = \u001b[43mtf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcontrib\u001b[49m.distributions\n\u001b[32m     33\u001b[39m \u001b[38;5;66;03m# TensorFlow's default implementation of the KL divergence between two\u001b[39;00m\n\u001b[32m     34\u001b[39m \u001b[38;5;66;03m# tf.contrib.distributions.MultivariateNormalDiag instances sometimes results\u001b[39;00m\n\u001b[32m     35\u001b[39m \u001b[38;5;66;03m# in NaN values in the gradients (not in the forward pass). Until the default\u001b[39;00m\n\u001b[32m     36\u001b[39m \u001b[38;5;66;03m# implementation is fixed, we use our own KL implementation.\u001b[39;00m\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mCustomKLDiagNormal\u001b[39;00m(tfd.MultivariateNormalDiag):\n",
      "\u001b[31mAttributeError\u001b[39m: module 'tensorflow' has no attribute 'contrib'"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Literal\n",
    "\n",
    "from agents import Agent, ItemHelpers, Runner, TResponseInputItem, trace\n",
    "\n",
    "\"\"\"\n",
    "This example shows the agents collaborating or one agent reveiwing the work of another and giving feedback. \n",
    "The first agent generates an outline for a story.\n",
    "The second agent judges the outline and provides feedback. \n",
    "We loop until the judge is satisfied with the outline.\n",
    "\"\"\"\n",
    "# model = \"gpt-4o\"\n",
    "\n",
    "story_outline_generator = Agent(\n",
    "    name=\"story_outline_generator\",\n",
    "    instructions=(\n",
    "        \"You generate a very short story outline based on the user's input.\"\n",
    "        \"If there is any feedback provided, use it to improve the outline.\"\n",
    "    ),\n",
    "    model= model,\n",
    ")\n",
    "\n",
    "@dataclass\n",
    "class EvaluationFeedback:\n",
    "    feedback: str\n",
    "    score: Literal[\"pass\", \"needs_improvement\", \"fail\"]\n",
    "\n",
    "\n",
    "evaluator = Agent(\n",
    "    name=\"evaluator\",\n",
    "    instructions=(\n",
    "        \"You evaluate a story outline and decide if it's good enough.\"\n",
    "        \"If it's not good enough, you provide feedback on what needs to be improved.\"\n",
    "    ),\n",
    "    model= model,\n",
    "    output_type=EvaluationFeedback,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d01d3ea3-ec28-447a-9fd3-41f54ecc9650",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What kind of story would you like to hear?  joyful story\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Story outline generated\n",
      "Evaluator score: pass\n",
      "Story outline is good enough, exiting.\n",
      "Final story outline: Title: **The Day the Music Danced**\n",
      "\n",
      "**Outline:**\n",
      "\n",
      "1. **Introduction:**\n",
      "   - In the quaint village of Harmonia, every Sunday, the townsfolk gather for a vibrant music festival.\n",
      "   - The protagonist, Lily, a cheerful young girl with a talent for playing the violin, eagerly anticipates this day.\n",
      "\n",
      "2. **Rising Action:**\n",
      "   - This year, a surprise visitor arrives—a wise old musician named Maestro Anton, known for his magical melodies that can make anything dance.\n",
      "   - Lily is chosen to play a duet with him, feeling both excited and nervous.\n",
      "\n",
      "3. **Climax:**\n",
      "   - As they begin to play, the music comes to life, lifting everyone’s spirits.\n",
      "   - The notes swirl in the air, causing trees to sway and flowers to bloom in rhythm, creating a whimsical dance scene.\n",
      "\n",
      "4. **Falling Action:**\n",
      "   - The joyful dance envelops the village, bringing together people of all ages in harmony.\n",
      "   - Lily’s confidence soars as she realizes the true power of her music—to unite and uplift.\n",
      "\n",
      "5. **Conclusion:**\n",
      "   - The festival ends with laughter and gratitude, and Lily receives a small charm from Maestro Anton as a reminder of their special day.\n",
      "   - Lily vows to continue sharing her joyful music with the world, knowing it can paint smiles and create memories.\n",
      "\n",
      "**Feedback Improvements:**\n",
      "If needed, the outline can focus more on specific interactions between Lily and the villagers to deepen character connections or include more sensory details to enrich the atmosphere.\n"
     ]
    }
   ],
   "source": [
    "msg = input(\"What kind of story would you like to hear? \")\n",
    "input_items: list[TResponseInputItem] = [{\"content\": msg, \"role\": \"user\"}]\n",
    "\n",
    "latest_outline: str | None = None\n",
    "\n",
    "with trace(\"Collaboration\"):\n",
    "        i = 0\n",
    "        while True:\n",
    "            story_outline_result = await Runner.run(\n",
    "                story_outline_generator,\n",
    "                input_items,\n",
    "            )\n",
    "\n",
    "            input_items = story_outline_result.to_input_list()\n",
    "            latest_outline = ItemHelpers.text_message_outputs(story_outline_result.new_items)\n",
    "            print(\"Story outline generated\")\n",
    "\n",
    "            evaluator_result = await Runner.run(evaluator, input_items)\n",
    "            result: EvaluationFeedback = evaluator_result.final_output\n",
    "\n",
    "            print(f\"Evaluator score: {result.score}\")\n",
    "\n",
    "            if result.score == \"pass\":\n",
    "                print(\"Story outline is good enough, exiting.\")\n",
    "                break\n",
    "            if i == 2:\n",
    "                print(\"Maximum number of iterations exceeded, exiting.\")\n",
    "                break\n",
    "            print(\"Re-running with feedback\")\n",
    "\n",
    "            input_items.append({\"content\": f\"Feedback: {result.feedback}\", \"role\": \"user\"})\n",
    "            i += 1\n",
    "\n",
    "print(f\"Final story outline: {latest_outline}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af63f69-2d06-40f9-9fa5-cc490cf11768",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Agents routing\n",
    "1. Simply demonstrates an agent routing work to other agents.\n",
    "1. This is a very common agentic pattern.\n",
    "1. Ask the question in German and see what happens! In real life when we use a routing pattern, we must have a fallback agent that gracefully handles all things unknown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fa28f4d9-5ab4-41f8-8658-ece1d3e31de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import uuid\n",
    "\n",
    "#from openai.types.responses import ResponseContentPartDoneEvent, ResponseTextDeltaEvent\n",
    "\n",
    "from agents import Agent, RawResponsesStreamEvent, Runner, TResponseInputItem, trace\n",
    "\n",
    "\"\"\"\n",
    "This example shows the handoffs/routing pattern. The triage agent receives the first message, and\n",
    "then hands off to the appropriate agent based on the language of the request. Responses are\n",
    "streamed to the user.\n",
    "\"\"\"\n",
    "model = \"gpt-4o\"\n",
    "\n",
    "french_agent = Agent(\n",
    "    name=\"french_agent\",\n",
    "    instructions=\"You only speak French\",\n",
    "    model = model,\n",
    ")\n",
    "\n",
    "spanish_agent = Agent(\n",
    "    name=\"spanish_agent\",\n",
    "    instructions=\"You only speak Spanish\",\n",
    "    model = model,\n",
    ")\n",
    "\n",
    "english_agent = Agent(\n",
    "    name=\"english_agent\",\n",
    "    instructions=\"You only speak English. Answer the question you recieved.\",\n",
    "    model = model,\n",
    ")\n",
    "\n",
    "# experiment by removing  from the instructions the sentence below.\n",
    "#So answer in English even if you understand the language that is being used.\n",
    "#And then ask say (German) : Wie geht es dir\n",
    "know_all_agent = Agent(\n",
    "    name=\"know_all_agent\",\n",
    "    instructions=\"You only speak English. So answer in English even if you understand the language that is being used. \\\n",
    "        State that you do not understand the user question and ask them to repeat it one of the languages you understand. \\\n",
    "        Those languages are English, French and Spanish .\",\n",
    "    model = model,\n",
    ")\n",
    "\n",
    "triage_agent = Agent(\n",
    "    name=\"triage_agent\",\n",
    "    instructions=\"Handoff to the appropriate agent based on the language of the request. If you do not know what to do, hand if off to know_all.\",\n",
    "    handoffs=[french_agent, spanish_agent, english_agent, know_all_agent],\n",
    "    model = model,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c8bbe4d0-3806-4b29-9f28-6e10361abf88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi! We speak French, Spanish and English. How can I help?  bonjour monsieur\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------\n",
      "Bonjour ! Comment puis-je vous aider aujourd'hui ?\n"
     ]
    }
   ],
   "source": [
    "\n",
    "msg = input(\"Hi! We speak French, Spanish and English. How can I help? \")\n",
    "inputs: list[TResponseInputItem] = [{\"content\": msg, \"role\": \"user\"}]\n",
    "\n",
    "with trace(\"Router\"):\n",
    "    story_outline_result = await Runner.run(triage_agent,inputs)\n",
    "    #uncomment this to see the details\n",
    "    #pprint(story_outline_result)\n",
    "    print(\"--------------------------\")\n",
    "    print(story_outline_result.final_output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3660bec5-d54d-473f-9f03-0604120fdbd1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Agents Deterministic Workflow\n",
    "1. Simply demonstrates agents calling other agents to complete a well defined workflow.\n",
    "1. This is a very common agentic pattern.\n",
    "1. This pattern or its variants can be put to lot of practical use and it could be combined with the collaborative pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8ef7aa00-e4b1-4b50-ba19-c428c6dbb277",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Literal\n",
    "\n",
    "from agents import Agent, ItemHelpers, Runner, TResponseInputItem, trace\n",
    "\n",
    "\"\"\"\n",
    "This example shows how different agents are used to compelete a deterministic workflow.\n",
    "In this case it is: \n",
    "planner agent -> writer agent -> editor agent \n",
    "Given an essay topic, the essay moves through these stages to finally produce an output.\n",
    "\"\"\"\n",
    "model = \"gpt-4o\"\n",
    "\n",
    "@dataclass\n",
    "class Planner:\n",
    "    body: str\n",
    "\n",
    "@dataclass\n",
    "class Writer:\n",
    "    body: str\n",
    "\n",
    "@dataclass\n",
    "class Editor:\n",
    "    body: str\n",
    "\n",
    "\n",
    "\n",
    "planner_agent = Agent(\n",
    "    name=\"planner_agent\",\n",
    "    instructions=(\n",
    "        \"Take a user's theme/topic request.\"\n",
    "        \"Create a brief outline of the essay with points that need to be covered.\"\n",
    "        \"Make sure that references are given to actual source materials.\"\n",
    "    ),\n",
    "    model= model,\n",
    "    output_type=Planner,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "writer_agent = Agent(\n",
    "    name=\"writer_agent\",\n",
    "    instructions=(\n",
    "        \"Take the outline given in the input.\"\n",
    "        \"Expands it into a complete essay.\"\n",
    "        \"Give adequate references and make sure things are not made up!\"\n",
    "    ),\n",
    "    model= model,\n",
    "    output_type=Writer,\n",
    ")\n",
    "\n",
    "editor_agent = Agent(\n",
    "    name=\"editor_agent\",\n",
    "    instructions=(\n",
    "        \"You Review the draft given in the input.\"\n",
    "        \"Polish the language, fixes inconsistencies, and improve the flow.\"\n",
    "        \"And make sure it is logical coherent.\"\n",
    "        \"Give adequate references and make sure things are not made up!\"\n",
    "        \"Return the final story to the user.\"\n",
    "    ),\n",
    "    model= model,\n",
    "    output_type=Editor,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0c2656ad-540c-4049-892e-a0dde674962b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi! I am AI Researcher. Give me any topic and I will write a well researched essay about it.  Model Context Protocol Servers and tools\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------Planner Output----------\n",
      "### Essay Outline: Model Context Protocol Servers and Tools\n",
      "\n",
      "#### Introduction\n",
      "- Define Model Context Protocol (MCP) and its significance in software development.\n",
      "- Brief introduction to protocol servers and tools.\n",
      "- Importance of context-aware computing.\n",
      "  \n",
      "#### Background of MCP\n",
      "- Origin and development of MCP.\n",
      "- Key features and functionalities.\n",
      "- Comparison with other protocols (e.g., HTTP, FTP).\n",
      "\n",
      "#### Architecture of MCP Servers\n",
      "- Overview of server architecture.\n",
      "- Role of context information in MCP.\n",
      "- How MCP servers handle context data efficiently.\n",
      "  \n",
      "#### Tools for MCP\n",
      "- Popular tools used in MCP implementation.\n",
      "  - MCP Toolkits.\n",
      "  - Libraries and frameworks.\n",
      "- Criteria for selecting appropriate tools.\n",
      "\n",
      "#### Applications\n",
      "- Use cases of MCP in various industries.\n",
      "  - Healthcare.\n",
      "  - IoT.\n",
      "- Efficiency improvements in data handling and processing.\n",
      "\n",
      "#### Challenges and Solutions\n",
      "- Common challenges faced in MCP implementation.\n",
      "- Proposed solutions.\n",
      "- Future developments and research directions.\n",
      "\n",
      "#### Conclusion\n",
      "- Summary of key points.\n",
      "- The potential impact of MCP in the future.\n",
      "\n",
      "#### References\n",
      "- **Reference 1:** [Title of the Source]. (Year). Retrieved from [URL].\n",
      "- **Reference 2:** [Author, Book/Article Title]. (Publisher, Year).\n",
      "- **Reference 3:** [Title of the Source]. (Year). Retrieved from [URL].\n",
      "\n",
      "### Additional Notes\n",
      "- Use credible sources such as academic journals, books, and verified websites.\n",
      "- Include recent developments and updates in MCP technology.\n",
      "- Ensure all references are in the correct format and easily accessible for further exploration.\n",
      "----------Writer Output----------\n",
      "### Essay: Model Context Protocol Servers and Tools\n",
      "\n",
      "#### Introduction\n",
      "In the ever-evolving landscape of software development, the Model Context Protocol (MCP) has emerged as a pivotal element in fostering context-aware computing. Its significance lies in enhancing the adaptability and intelligence of software systems to react appropriately in varied environments. Protocol servers and tools play a crucial role in managing and facilitating these interactions effectively, thereby underscoring the importance of context-aware technology in modern computing.\n",
      "\n",
      "#### Background of MCP\n",
      "The Model Context Protocol originated from the need to provide a more structured and efficient way to manage context data in complex systems. Unlike traditional protocols like HTTP and FTP, which primarily focus on facilitating data exchange, MCP emphasizes the importance of context in interpreting data meaningfully. MCP enables systems to modify their operations based on the contexts they perceive, making them inherently smarter and more responsive. Key features include the ability to seamlessly integrate with various data sources, manage contextual changes dynamically, and offer robust scalability to fit diverse applications.\n",
      "\n",
      "#### Architecture of MCP Servers\n",
      "MCP server architecture is pivotal in managing context information. Typically, these servers are designed to intake diverse context data, process it in real-time, and adapt the system’s behavior accordingly. The architecture supports distributed data sources, where context data is efficiently gathered and processed. This ensures the seamless handling of rapid context changes, which is crucial in high-stakes environments such as real-time IoT applications or healthcare systems. Efficient context data handling in MCP servers reduces latency, enhances accuracy, and optimizes resource use.\n",
      "\n",
      "#### Tools for MCP\n",
      "Numerous tools aid in the effective implementation of MCP. MCP Toolkits are immensely popular, providing developers with a framework to build, manage, and deploy context-aware systems. Libraries and frameworks such as Context Toolkit and CoBra are instrumental in offering structured approaches to managing context data. In selecting appropriate tools, criteria such as ease of integration, scalability, support for various data inputs, and real-time processing capabilities are paramount. These tools simplify the complex processes involved in managing and interpreting context data.\n",
      "\n",
      "#### Applications\n",
      "MCP finds varied applications across industries. In healthcare, MCP can dramatically improve patient care through real-time data processing and decision-making support, adjusting to vital signs and environmental factors. In the IoT sector, MCP enhances device interconnectivity, enabling devices to adapt operations based on current conditions dynamically. Such applications underscore the protocol’s capacity to substantially improve efficiency in data handling and processing, leading to profound operational improvements.\n",
      "\n",
      "#### Challenges and Solutions\n",
      "Implementing MCP is not without challenges. Common issues include managing the vast diversity of context data sources and ensuring data privacy and security. Addressing these requires robust encryption methods and flexible, scalable architectures that can accommodate future demand. Emerging research suggests adopting hybrid models that combine centralized and decentralized processing could offer viable solutions to these challenges. Ongoing advancements in artificial intelligence and machine learning are also poised to augment the capabilities of MCP, offering promising directions for future development.\n",
      "\n",
      "#### Conclusion\n",
      "In conclusion, the Model Context Protocol significantly elevates the potential of software systems to operate contextually. By enabling systems to interpret and respond to real-world data dynamically, MCP is set to redefine many aspects of technology use across industries. Its future impact, bolstered by ongoing advancements and research, promises to further revolutionize the fields of IoT, healthcare, and beyond.\n",
      "\n",
      "#### References\n",
      "- **Reference 1:** [Bettini, C., Brdiczka, O., Henricksen, K., et al.: A survey of context modelling and reasoning techniques. Pervasive and Mobile Computing 6(2), 161–180 (2010).](https://www.sciencedirect.com/science/article/abs/pii/S1574119210000159)\n",
      "- **Reference 2:** [Dey, A. K., & Abowd, G. D. (2000). Towards a better understanding of context and context-awareness. Proceedings of the 2000 Workshop on the What, Who, Where, When, and How of Context-Awareness in Pervasive Computing (CHI 2000).](https://citeseerx.ist.psu.edu/document?doi=10.1.1.110.7723)\n",
      "- **Reference 3:** [Perera, C., Zaslavsky, A., Christen, P., & Georgakopoulos, D. (2014). Context-aware computing for the internet of things: A survey. IEEE Communications Surveys & Tutorials, 16(1), 414-454.](https://ieeexplore.ieee.org/document/6704472)\n",
      "\n",
      "### Additional Notes\n",
      "Continual advancements in MCP technology require attention to contemporary research and updates. Ensuring all references adhere to proper format aids in facilitating further exploration for interested audiences. Aligning MCP developments with emerging technology trends amplifies its relevance and application potential across varied fields.\n",
      "----------Editor Output----------\n",
      "### Essay: Model Context Protocol Servers and Tools\n",
      "\n",
      "#### Introduction\n",
      "In the rapidly advancing field of software development, the Model Context Protocol (MCP) has emerged as a critical component in promoting context-aware computing. Its importance stems from its ability to enhance the intelligence and adaptability of software systems, enabling them to respond effectively in diverse environments. Protocol servers and tools are essential in managing and facilitating these interactions, highlighting the significance of context-aware technology in contemporary computing.\n",
      "\n",
      "#### Background of MCP\n",
      "The Model Context Protocol was developed to provide a more structured and efficient approach to managing context data in complex systems. Unlike traditional protocols such as HTTP and FTP, which primarily focus on data exchange, MCP emphasizes the importance of context in interpreting data meaningfully. MCP allows systems to modify their operations based on perceived contexts, making them inherently smarter and more responsive. Key features include seamless integration with various data sources, dynamic management of contextual changes, and robust scalability to accommodate diverse applications.\n",
      "\n",
      "#### Architecture of MCP Servers\n",
      "The architecture of MCP servers is crucial in managing context information. These servers are typically designed to intake diverse context data, process it in real-time, and adapt the system's behavior accordingly. The architecture supports distributed data sources, efficiently gathering and processing context data. This ensures the seamless handling of rapid context changes, which is vital in high-stakes environments such as real-time IoT applications or healthcare systems. Efficient context data management in MCP servers reduces latency, enhances accuracy, and optimizes resource use.\n",
      "\n",
      "#### Tools for MCP\n",
      "Various tools facilitate the effective implementation of MCP. MCP toolkits are popular, providing developers with frameworks to build, manage, and deploy context-aware systems. Libraries and frameworks such as the Context Toolkit and CoBra are instrumental in offering structured approaches to managing context data. Selecting appropriate tools involves considering criteria such as ease of integration, scalability, support for various data inputs, and real-time processing capabilities. These tools simplify the complex processes involved in managing and interpreting context data.\n",
      "\n",
      "#### Applications\n",
      "MCP has diverse applications across industries. In healthcare, it can significantly improve patient care through real-time data processing and decision-making support, adapting to vital signs and environmental factors. In the IoT sector, MCP enhances device interconnectivity, allowing devices to dynamically adapt operations based on current conditions. These applications highlight the protocol's capacity to improve efficiency in data handling and processing, leading to substantial operational enhancements.\n",
      "\n",
      "#### Challenges and Solutions\n",
      "Implementing MCP presents challenges, including managing the diversity of context data sources and ensuring data privacy and security. Addressing these issues requires robust encryption methods and flexible, scalable architectures that can accommodate future demand. Emerging research suggests adopting hybrid models that combine centralized and decentralized processing as viable solutions. Ongoing advancements in artificial intelligence and machine learning are also set to augment MCP's capabilities, offering promising directions for future development.\n",
      "\n",
      "#### Conclusion\n",
      "In conclusion, the Model Context Protocol significantly elevates the potential of software systems to operate contextually. By enabling systems to interpret and respond to real-world data dynamically, MCP is poised to redefine many aspects of technology use across industries. Its future impact, supported by ongoing advancements and research, promises to further transform fields like IoT, healthcare, and beyond.\n",
      "\n",
      "#### References\n",
      "- **Bettini, C., Brdiczka, O., Henricksen, K., et al.:** *A survey of context modelling and reasoning techniques.* Pervasive and Mobile Computing 6(2), 161–180 (2010). [Link](https://www.sciencedirect.com/science/article/abs/pii/S1574119210000159)\n",
      "- **Dey, A. K., & Abowd, G. D. (2000):** *Towards a better understanding of context and context-awareness.* Proceedings of the 2000 Workshop on the What, Who, Where, When, and How of Context-Awareness in Pervasive Computing (CHI 2000). [Link](https://citeseerx.ist.psu.edu/document?doi=10.1.1.110.7723)\n",
      "- **Perera, C., Zaslavsky, A., Christen, P., & Georgakopoulos, D. (2014):** *Context-aware computing for the internet of things: A survey.* IEEE Communications Surveys & Tutorials, 16(1), 414-454. [Link](https://ieeexplore.ieee.org/document/6704472)\n",
      "\n",
      "### Additional Notes\n",
      "Continuous advancements in MCP technology necessitate attention to contemporary research and updates. Ensuring all references follow the appropriate format aids exploration for interested audiences. Aligning MCP developments with emerging technology trends enhances its relevance and application potential across various fields.\n"
     ]
    }
   ],
   "source": [
    "msg = input(\"Hi! I am AI Researcher. Give me any topic and I will write a well researched essay about it. \")\n",
    "inputs: list[TResponseInputItem] = [{\"content\": msg, \"role\": \"user\"}]\n",
    "\n",
    "with trace(\"Workflow\"):\n",
    "    print(\"----------Planner Output----------\")\n",
    "    planner_result = await Runner.run(planner_agent,inputs)\n",
    "    print(planner_result.final_output.body)\n",
    "    planner_output: Planner = planner_result.final_output\n",
    "    print(\"----------Writer Output----------\")\n",
    "    writer_result = await Runner.run(writer_agent,planner_output.body)\n",
    "    print(writer_result.final_output.body)\n",
    "    writer_output: Writer = writer_result.final_output\n",
    "    print(\"----------Editor Output----------\")\n",
    "    editor_result = await Runner.run(editor_agent,writer_output.body)\n",
    "    print(editor_result.final_output.body)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38efe065-d4e6-41a6-bb4b-60ce7734f102",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Microservices\n",
    "\n",
    "There are several meaningful similarities between LLM-based AI agents and microservices:\n",
    "\n",
    "## Similarities\n",
    "#### Specialized functionality: \n",
    "Both are designed to handle specific tasks or domains. Microservices focus on particular business capabilities, while AI agents can be specialized for specific types of interactions or knowledge domains.\n",
    "### Independent operation: \n",
    "Both can operate autonomously within their defined scope. Once configured, they can process requests without requiring constant supervision.\n",
    "### Communication patterns: \n",
    "Both typically communicate via messages/APIs. Microservices use REST/gRPC/messaging protocols, while AI agents receive prompts and return responses through APIs.\n",
    "### Composability: \n",
    "Both can be combined to build larger systems. Microservices can be orchestrated to create complex applications; similarly, multiple AI agents can work together in a workflow.\n",
    "### Statelessness vs. statefulness: \n",
    "Basic implementations of both can be stateless, but more sophisticated versions maintain state. The Agent class you showed maintains conversation history, similar to how some microservices maintain session state.\n",
    "### Scaling considerations: \n",
    "Both face similar operational challenges around scaling, monitoring, and versioning.\n",
    "\n",
    "## Key differences:\n",
    "\n",
    "### Implementation: \n",
    "Microservices are traditional code with deterministic logic, while LLM agents use probabilistic models. MCP Servers which expose tools to be used by Agents could be totally traditional code with deterministic logic.\n",
    "### Predictability: \n",
    "Microservices have more predictable outputs for given inputs, while LLM responses can vary.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b9ab57-7246-4c17-a9c1-2ccc9a7cc487",
   "metadata": {},
   "source": [
    "# AFTERWORD\n",
    "Agents are an extremely powerful construct in the field of Generative AI:\n",
    "1. You can achieve complex tasks designing appropriate agents and tools and driving interaction between the different agents.\n",
    "1. There are known ways by which we can improve accuracy of the output. Much like human beings help check one another's work, agents can do the same.\n",
    "1. External data retrieval and queries are carried out through the tools.\n",
    "1. If agent processing needs to be vetted, make sure humans are used (human-in-the-loop) to are used to vet the agent output before it moves to the next step. Really, this is no different to how we operate in our real life with human beings - we have review and approval processes etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78716910-84ca-42b6-9f2b-faa39f4df1fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "summit-agentic-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
